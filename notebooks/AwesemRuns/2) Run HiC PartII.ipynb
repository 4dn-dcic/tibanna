{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "#format for input json in hic-partII\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    if not isinstance(obj_ids, list):\n",
    "        obj_ids = [ obj_ids ]     \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "         \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])     \n",
    "    if len(uuid_list)==1:\n",
    "        uuid_list = uuid_list[0]\n",
    "    if len(object_key_list)==1:\n",
    "        object_key_list = object_key_list[0]  \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "def make_hic2_json(input_files, env, output_bucket, accession):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"65586d4b-1e3b-4b31-891e-11f48c816545\",\n",
    "                  \"app_name\": \"pairsam-parse-sort\",\n",
    "                  \"parameters\": {\n",
    "                      \"nThreads\": 1\n",
    "                      },\n",
    "                  \"config\": {\n",
    "                      \"ebs_type\": \"io1\",\n",
    "                      \"json_bucket\": \"4dn-aws-pipeline-run-json\",\n",
    "                      \"ebs_iops\": 500,\n",
    "                      \"shutdown_min\": 30,\n",
    "                      \"s3_access_arn\": \"arn:aws:iam::643366669028:instance-profile/S3_access\",\n",
    "                      \"ami_id\": \"ami-7ff26968\",\n",
    "                      \"copy_to_s3\": True,\n",
    "                      \"script_url\": \"https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/\",\n",
    "                      \"launch_instance\": True,\n",
    "                      \"password\": \"hahaha\",\n",
    "                      \"log_bucket\": \"tibanna-output\"\n",
    "                    },\n",
    "                  \"_tibanna\": {\"env\": env, \n",
    "                               \"run_type\": \"pairsam-parse-sort\",\n",
    "                               \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcic:Selvaraj_gm12878_hic\n",
      "4DNFI5VLWJVD 4DNFI9RZ5M46\n",
      "bam file is 4DNFIKL2V1JD\n",
      "\n",
      "4DNFISFTHGMO 4DNFIRHB1DY6\n",
      "no bam file for paired fastq files 4DNFISFTHGMO 4DNFIRHB1DY6\n",
      "\n",
      "4DNFIFLPPFEF 4DNFIEOF23ON\n",
      "no bam file for paired fastq files 4DNFIFLPPFEF 4DNFIEOF23ON\n",
      "\n",
      "4DNFIF682T66 4DNFIX3JHRA6\n",
      "bam file is 4DNFIMCX3ZTZ\n",
      "\n",
      "4DNFIBFNQAPD 4DNFI3B5A5F7\n",
      "bam file is 4DNFIYY9N5TP\n",
      "\n",
      "4DNFIPST6TJR 4DNFIBA9UA7A\n",
      "no bam file for paired fastq files 4DNFIPST6TJR 4DNFIBA9UA7A\n",
      "\n",
      "4DNFIWTWWMFV 4DNFIDJTCT3M\n",
      "no bam file for paired fastq files 4DNFIWTWWMFV 4DNFIDJTCT3M\n",
      "\n",
      "4DNFIBW7YMZK 4DNFIJT9H12M\n",
      "no bam file for paired fastq files 4DNFIBW7YMZK 4DNFIJT9H12M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from invoke import run\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "all_sets = [\n",
    "            'dcic:Selvaraj_gm12878_hic',\n",
    "            'dekker-lab:ExperimentSet_U54_U54-ESC4DN-FA-DpnII-2017524'\n",
    "            ]\n",
    "   \n",
    "my_rep_set = all_sets[0]\n",
    "print my_rep_set\n",
    "\n",
    "wf_partI = \"bwa-mem\"\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "rep_set_resp = ff_utils.get_metadata(my_rep_set, connection=ff)\n",
    "rep_resp = rep_set_resp['experiments_in_set']\n",
    "set_acc = rep_set_resp['accession']\n",
    "\n",
    "bams = []\n",
    "for exp in rep_resp:\n",
    "    # print 'Experiment', exp\n",
    "    exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "    exp_files = exp_resp['files']\n",
    "    for fastq_file in exp_files:\n",
    "        file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded')  \n",
    "        #Some checks before running\n",
    "        #check if status is deleted\n",
    "        if file_resp['status'] == 'deleted':\n",
    "            print \"delete file\", file_resp['accession']\n",
    "            continue\n",
    "        #if no uploaded file in the file item report and skip\n",
    "        if not file_resp.get('filename'):\n",
    "            print file_resp['accession'], \"does not have a file\"\n",
    "            continue\n",
    "        # check if file is in s3\n",
    "        head_info = tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket)\n",
    "        if not head_info:\n",
    "            print file_resp['accession'], \"does not have a file in S3\"\n",
    "            continue\n",
    "        \n",
    "        # skip pair no 2\n",
    "        if file_resp.get('paired_end')=='2':\n",
    "            continue\n",
    "        \n",
    "        paired_file = file_resp['related_files'][0]['file']['accession']\n",
    "        print file_resp['accession'], paired_file\n",
    "        \n",
    "        workflows = file_resp.get('workflow_run_inputs')\n",
    "        partI_wfr = {}\n",
    "        partI_status = 'did not run'\n",
    "        if workflows:\n",
    "            for a_wfr in workflows:\n",
    "                wfr_resp = ff_utils.get_metadata(a_wfr['uuid'], connection=ff)  \n",
    "                wfr_name = wfr_resp['display_title']\n",
    "                if wfr_name.startswith(wf_partI):\n",
    "                    partI_wfr = wfr_resp\n",
    "                    partI_status = wfr_resp['run_status']\n",
    "\n",
    "        if partI_status == 'complete':\n",
    "            bam_outputs = partI_wfr.get('output_files')\n",
    "            bam_file = [i['value'] for i in bam_outputs if i['format'] == 'bam'][0]\n",
    "            bam_resp = ff_utils.get_metadata(bam_file, connection=ff)\n",
    "            print 'bam file is', bam_resp['accession']\n",
    "        else:\n",
    "            print 'no bam file for paired fastq files', file_resp['accession'], paired_file\n",
    "        print\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for upload key 1f53df95-4cf3-41cc-971d-81bb16c486dd/4DNFIZQZ39L9.bwaIndex.tgz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key e893d235-708b-4a33-bd29-86103c310718/4DNFIWTWWMFV.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 3ddabeaa-c752-4097-a519-3f53caafdd6b/4DNFIDJTCT3M.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFIWTWWMFV-4DNFIDJTCT3Mab007193-c9a5-4e17-a569-b4b216479b84\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2017, 9, 13, 16, 41, 42, 931000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': 'f3795846-98c3-11e7-b873-eda4ad7905af', 'HTTPHeaders': {'x-amzn-requestid': 'f3795846-98c3-11e7-b873-eda4ad7905af', 'content-length': '196', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:bwa-mem_4DNFIWTWWMFV-4DNFIDJTCT3Mab007193-c9a5-4e17-a569-b4b216479b84'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:bwa-mem_4DNFIWTWWMFV-4DNFIDJTCT3Mab007193-c9a5-4e17-a569-b4b216479b84\n",
      "Press Enter to continue...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "paired_files = [('4DNFIWTWWMFV','4DNFIDJTCT3M')]\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "outfiles = tibanna.s3.outfile_bucket\n",
    "tibanna.s3.outfile_bucket = 'elasticbeanstalk-fourfront-webprod-files'\n",
    "\n",
    "# todo need a function to determin this given fastq1\n",
    "index = make_input_file_json('4DNFIZQZ39L9', 'bwa_index', tibanna)\n",
    "\n",
    "for pair in paired_files:\n",
    "    fastq1 = make_input_file_json(pair[0], 'fastq1', tibanna)\n",
    "    fastq2 = make_input_file_json(pair[1], 'fastq2', tibanna)\n",
    "    \n",
    "    input_files = [fastq1, fastq2, index]\n",
    "    if all(input_files):\n",
    "        name = fastq1['object_key'].split('.')[0] + \"-\" + fastq2['object_key'].split('.')[0]\n",
    "        input_json = make_hic1_json(input_files, env, outfiles, name)\n",
    "        #print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    #time.sleep(5)\n",
    "    a = raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
