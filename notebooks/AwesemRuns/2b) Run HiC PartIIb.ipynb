{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "#format for input json in hic-partII\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    if not isinstance(obj_ids, list):\n",
    "        obj_ids = [ obj_ids ]     \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "         \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])     \n",
    "    if len(uuid_list)==1:\n",
    "        uuid_list = uuid_list[0]\n",
    "    if len(object_key_list)==1:\n",
    "        object_key_list = object_key_list[0]  \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "def make_hic2b_json(input_files, env, output_bucket, accession):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"af8908bf-fdcb-40be-8bca-f1a49226bd20\",\n",
    "                  \"app_name\": \"pairsam-merge\",\n",
    "                  \"parameters\": {\n",
    "                      \"nThreads\": 1\n",
    "                      },\n",
    "                    \"config\": {\n",
    "                        \"ebs_type\": \"io1\",\n",
    "                        \"json_bucket\": \"4dn-aws-pipeline-run-json\",\n",
    "                        \"EBS_optimized\": True,\n",
    "                        \"ebs_iops\": 5000,\n",
    "                        \"shutdown_min\": 30,\n",
    "                        \"instance_type\": \"m4.16xlarge\",\n",
    "                        \"s3_access_arn\": \"arn:aws:iam::643366669028:instance-profile/S3_access\",\n",
    "                        \"ami_id\": \"ami-cfb14bb5\",\n",
    "                        \"copy_to_s3\": True,\n",
    "                        \"script_url\": \"https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/\",\n",
    "                        \"launch_instance\": True,\n",
    "                        \"password\": \"hahaha\",\n",
    "                        \"log_bucket\": \"tibanna-output\",\n",
    "                        \"key_name\": \"4dn-encode\"\n",
    "                      },\n",
    "                  \"_tibanna\": {\"env\": env, \n",
    "                               \"run_type\": \"pairsam-merge\",\n",
    "                               \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "\n",
    "def get_wfr_out(emb_file, wfr_name, file_format):\n",
    "    workflows = emb_file.get('workflow_run_inputs')\n",
    "    wfr = {}\n",
    "    run_status = 'did not run'\n",
    "    if workflows:\n",
    "        for a_wfr in workflows:\n",
    "            wfr_resp = ff_utils.get_metadata(a_wfr['uuid'], connection=ff)  \n",
    "            wfr_resp_name = wfr_resp['display_title']\n",
    "            if wfr_resp_name.startswith(wfr_name):\n",
    "                wfr = wfr_resp\n",
    "                run_status = wfr_resp['run_status']\n",
    "    else:\n",
    "        return \"no workflow in file\"\n",
    "    \n",
    "    if run_status == 'complete':\n",
    "        outputs = wfr.get('output_files')\n",
    "        file_id = [i['value'] for i in outputs if i['format'] == file_format][0]\n",
    "        if file_id:\n",
    "            return file_id\n",
    "        else:\n",
    "            return \"no file found\"\n",
    "    else:\n",
    "        return \"no completed run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dekker-lab:ExperimentSet_U54_HFFc6-FA-DpnII\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIRTAVPBQ\",\"4DNFIRTAVPBQ\")\tf2e463b5-6d81-41e3-a506-4777a75f3001\t37.05\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFINARWX11\",\"4DNFINARWX11\")\t48568d74-b775-4b41-ab4e-6f06a8e4a3e6\t35.23\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIN43BIDG\",\"4DNFIN43BIDG\")\tf82c0083-1680-4712-97a0-8bc80c592a16\t36.02\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFI8QXIKYI\",\"4DNFI8QXIKYI\")\ta3364d1c-dd37-423d-9397-305bbec7e74a\t34.76\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIV6SSSGF\",\"4DNFIV6SSSGF\")\t9d59f12f-d3c3-4c3b-9e75-76494b9635c2\t36.1\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFI65956HF\",\"4DNFI65956HF\")\t685e52ac-548f-4fbe-ad22-8ddc47620432\t35.66\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIVXVPPDK\",\"4DNFIVXVPPDK\")\ta1ac50bf-6050-431d-8a51-04fca0fb5af9\t34.61\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIYGIX22T\",\"4DNFIYGIX22T\")\t6b8c7971-aad5-480f-8254-cb36088161ea\t35.1\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIRQLKKXL\",\"4DNFIRQLKKXL\")\t31639ed9-966b-4947-a598-b083770df61c\t37.38\n",
      "same\n",
      "4DNEX7POCO84 has complete pairsem\n",
      "------------\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIPJMZ922\",\"4DNFIPJMZ922\")\t7b932aca-62f6-4d42-841b-0d7496567103\t37.15\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFI45RPI7R\",\"4DNFI45RPI7R\")\t03db448c-30db-4a49-9430-6bb05e642985\t35.12\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIWVWWZF9\",\"4DNFIWVWWZF9\")\t7039eae2-2beb-4b5c-8b37-ba3b91706b5a\t36.02\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIVT1N5YN\",\"4DNFIVT1N5YN\")\t4a1c61d4-a73c-49a4-bcb4-8f11035e80d4\t34.56\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIQP8KWJR\",\"4DNFIQP8KWJR\")\tf4664b6c-026c-4a63-954d-560138b882e2\t33.88\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIFD23W7Z\",\"4DNFIFD23W7Z\")\t7cfab89a-267c-4131-b701-a490c5ce8f41\t36.44\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFICHFDB9J\",\"4DNFICHFDB9J\")\t39258444-cfe3-4431-9464-bf66a1e3328a\t33.93\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFIR7BHTXP\",\"4DNFIR7BHTXP\")\t0ce61fc8-333d-41a0-b583-16c4b7aec0d1\t35.24\n",
      "same\n",
      "=HYPERLINK(\"https://data.4dnucleome.org/4DNFII9KPR27\",\"4DNFII9KPR27\")\t06d5b96f-6a60-4d79-81c8-406ab6808d63\t36.78\n",
      "same\n",
      "4DNEXRAEERUF has complete pairsem\n",
      "------------\n",
      "18 fastq file pairs in the set\n"
     ]
    }
   ],
   "source": [
    "from invoke import run\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def form_hyp(id):\n",
    "    hyp = '=HYPERLINK(\"https://data.4dnucleome.org/{0}\",\"{0}\")'.format(id)\n",
    "    return hyp\n",
    "\n",
    "\n",
    "all_sets = [\n",
    "            'dcic:Selvaraj_gm12878_hic',\n",
    "            'dekker-lab:ExperimentSet_U54_U54-ESC4DN-FA-DpnII-2017524',\n",
    "            'dekker-lab:ExperimentSet_U54_HFFc6-FA-DpnII'\n",
    "            ]\n",
    "   \n",
    "my_rep_set = all_sets[2]\n",
    "print my_rep_set\n",
    "\n",
    "wf_partI = \"bwa-mem\"\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "rep_set_resp = ff_utils.get_metadata(my_rep_set, connection=ff)\n",
    "rep_resp = rep_set_resp['experiments_in_set']\n",
    "set_acc = rep_set_resp['accession']\n",
    "\n",
    "exps_pairsems = []\n",
    "f_pairs = 0\n",
    "for exp in rep_resp:    \n",
    "    # print 'Experiment', exp\n",
    "    exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "    exp_files = exp_resp['files']\n",
    "    exp_acc = exp_resp['accession']\n",
    "    \n",
    "    exp_pairsems = []\n",
    "    all_fine = True\n",
    "\n",
    "    for fastq_file in exp_files:\n",
    "        file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded')  \n",
    "        #Some checks before running\n",
    "        #check if status is deleted\n",
    "        if file_resp['status'] == 'deleted':\n",
    "            print \"delete file\", file_resp['accession']\n",
    "            continue\n",
    "        #if no uploaded file in the file item report and skip\n",
    "        if not file_resp.get('filename'):\n",
    "            print file_resp['accession'], \"does not have a file\"\n",
    "            continue\n",
    "        # check if file is in s3\n",
    "        head_info = tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket)\n",
    "        if not head_info:\n",
    "            print file_resp['accession'], \"does not have a file in S3\"\n",
    "            continue\n",
    "        \n",
    "        # skip pair no 2\n",
    "        if file_resp.get('paired_end')=='2':\n",
    "            continue\n",
    "        f_pairs += 1\n",
    "        paired_file = file_resp['related_files'][0]['file']['accession']\n",
    "        #print file_resp['accession'], paired_file,\n",
    "        \n",
    "        #Check for partI\n",
    "        bam_file = get_wfr_out(file_resp, \"bwa-mem\", 'bam')\n",
    "        if bam_file.startswith('no') or not bam_file:\n",
    "            print bam_file\n",
    "            all_fine = False\n",
    "            continue \n",
    "        else:\n",
    "            bam_resp = ff_utils.get_metadata(bam_file, connection=ff, frame='embedded')\n",
    "            \n",
    "            # Check for part II\n",
    "            pairsem_file = get_wfr_out(bam_resp, \"pairsam-parse-sort\", 'pairsam')\n",
    "            if pairsem_file.startswith('no') or not pairsem_file:\n",
    "                print pairsem_file\n",
    "                all_fine = False\n",
    "                continue\n",
    "            else:\n",
    "                pairsem_resp = ff_utils.get_metadata(pairsem_file, connection=ff)\n",
    "                #print 'pairsem file is', pairsem_resp['accession']\n",
    "                exp_pairsems.append(pairsem_resp['accession'])\n",
    "                \n",
    "                f_s = round(pairsem_resp['file_size']/(1024*1024*1024.0),2)\n",
    "                print form_hyp(pairsem_resp[\"accession\"])+'\\t'+pairsem_resp[\"uuid\"]+\"\\t\"+str(f_s)\n",
    "                print \"same\"\n",
    "                \n",
    "    if all_fine:\n",
    "        exps_pairsems.append([exp_acc,exp_pairsems])\n",
    "        print exp_acc, \"has complete pairsem\"\n",
    "        print '------------'\n",
    "    else:\n",
    "        print exp_acc, \"has missing pairsem\"\n",
    "        print '------------'\n",
    "\n",
    "\n",
    "                  \n",
    "\n",
    "        \n",
    "        \n",
    "print f_pairs, \"fastq file pairs in the set\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for upload key f2e463b5-6d81-41e3-a506-4777a75f3001/4DNFIRTAVPBQ.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 48568d74-b775-4b41-ab4e-6f06a8e4a3e6/4DNFINARWX11.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key f82c0083-1680-4712-97a0-8bc80c592a16/4DNFIN43BIDG.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key a3364d1c-dd37-423d-9397-305bbec7e74a/4DNFI8QXIKYI.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 9d59f12f-d3c3-4c3b-9e75-76494b9635c2/4DNFIV6SSSGF.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 685e52ac-548f-4fbe-ad22-8ddc47620432/4DNFI65956HF.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key a1ac50bf-6050-431d-8a51-04fca0fb5af9/4DNFIVXVPPDK.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 6b8c7971-aad5-480f-8254-cb36088161ea/4DNFIYGIX22T.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 31639ed9-966b-4947-a598-b083770df61c/4DNFIRQLKKXL.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "{'app_name': 'pairsam-merge', 'parameters': {'nThreads': 1}, '_tibanna': {'run_type': 'pairsam-merge', 'env': 'fourfront-webprod', 'run_id': u'4DNEX7POCO84'}, 'output_bucket': 'elasticbeanstalk-fourfront-webprod-wfoutput', 'config': {'ebs_type': 'io1', 'EBS_optimized': True, 'ebs_iops': 5000, 'shutdown_min': 30, 's3_access_arn': 'arn:aws:iam::643366669028:instance-profile/S3_access', 'launch_instance': True, 'password': 'hahaha', 'log_bucket': 'tibanna-output', 'ami_id': 'ami-cfb14bb5', 'json_bucket': '4dn-aws-pipeline-run-json', 'instance_type': 'm4.16xlarge', 'copy_to_s3': True, 'script_url': 'https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/'}, 'workflow_uuid': 'af8908bf-fdcb-40be-8bca-f1a49226bd20', 'input_files': [{'workflow_argument_name': 'input_pairsams', 'bucket_name': 'elasticbeanstalk-fourfront-webprod-wfoutput', 'uuid': [u'f2e463b5-6d81-41e3-a506-4777a75f3001', u'48568d74-b775-4b41-ab4e-6f06a8e4a3e6', u'f82c0083-1680-4712-97a0-8bc80c592a16', u'a3364d1c-dd37-423d-9397-305bbec7e74a', u'9d59f12f-d3c3-4c3b-9e75-76494b9635c2', u'685e52ac-548f-4fbe-ad22-8ddc47620432', u'a1ac50bf-6050-431d-8a51-04fca0fb5af9', u'6b8c7971-aad5-480f-8254-cb36088161ea', u'31639ed9-966b-4947-a598-b083770df61c'], 'object_key': [u'4DNFIRTAVPBQ.sam.pairs.gz', u'4DNFINARWX11.sam.pairs.gz', u'4DNFIN43BIDG.sam.pairs.gz', u'4DNFI8QXIKYI.sam.pairs.gz', u'4DNFIV6SSSGF.sam.pairs.gz', u'4DNFI65956HF.sam.pairs.gz', u'4DNFIVXVPPDK.sam.pairs.gz', u'4DNFIYGIX22T.sam.pairs.gz', u'4DNFIRQLKKXL.sam.pairs.gz']}]}\n",
      "about to start run pairsam-merge_4DNEX7POCO84\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2017, 9, 16, 3, 11, 20, 860000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '3db4421d-9aae-11e7-9297-7347ed21e30e', 'HTTPHeaders': {'x-amzn-requestid': '3db4421d-9aae-11e7-9297-7347ed21e30e', 'content-length': '152', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:pairsam-merge_4DNEX7POCO84'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:pairsam-merge_4DNEX7POCO84\n",
      "looking for upload key 7b932aca-62f6-4d42-841b-0d7496567103/4DNFIPJMZ922.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 03db448c-30db-4a49-9430-6bb05e642985/4DNFI45RPI7R.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 7039eae2-2beb-4b5c-8b37-ba3b91706b5a/4DNFIWVWWZF9.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 4a1c61d4-a73c-49a4-bcb4-8f11035e80d4/4DNFIVT1N5YN.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key f4664b6c-026c-4a63-954d-560138b882e2/4DNFIQP8KWJR.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 7cfab89a-267c-4131-b701-a490c5ce8f41/4DNFIFD23W7Z.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 39258444-cfe3-4431-9464-bf66a1e3328a/4DNFICHFDB9J.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 0ce61fc8-333d-41a0-b583-16c4b7aec0d1/4DNFIR7BHTXP.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "looking for upload key 06d5b96f-6a60-4d79-81c8-406ab6808d63/4DNFII9KPR27.sam.pairs.gz, on bucket elasticbeanstalk-fourfront-webprod-wfoutput\n",
      "{'app_name': 'pairsam-merge', 'parameters': {'nThreads': 1}, '_tibanna': {'run_type': 'pairsam-merge', 'env': 'fourfront-webprod', 'run_id': u'4DNEXRAEERUF'}, 'output_bucket': 'elasticbeanstalk-fourfront-webprod-wfoutput', 'config': {'ebs_type': 'io1', 'EBS_optimized': True, 'ebs_iops': 5000, 'shutdown_min': 30, 's3_access_arn': 'arn:aws:iam::643366669028:instance-profile/S3_access', 'launch_instance': True, 'password': 'hahaha', 'log_bucket': 'tibanna-output', 'ami_id': 'ami-cfb14bb5', 'json_bucket': '4dn-aws-pipeline-run-json', 'instance_type': 'm4.16xlarge', 'copy_to_s3': True, 'script_url': 'https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/'}, 'workflow_uuid': 'af8908bf-fdcb-40be-8bca-f1a49226bd20', 'input_files': [{'workflow_argument_name': 'input_pairsams', 'bucket_name': 'elasticbeanstalk-fourfront-webprod-wfoutput', 'uuid': [u'7b932aca-62f6-4d42-841b-0d7496567103', u'03db448c-30db-4a49-9430-6bb05e642985', u'7039eae2-2beb-4b5c-8b37-ba3b91706b5a', u'4a1c61d4-a73c-49a4-bcb4-8f11035e80d4', u'f4664b6c-026c-4a63-954d-560138b882e2', u'7cfab89a-267c-4131-b701-a490c5ce8f41', u'39258444-cfe3-4431-9464-bf66a1e3328a', u'0ce61fc8-333d-41a0-b583-16c4b7aec0d1', u'06d5b96f-6a60-4d79-81c8-406ab6808d63'], 'object_key': [u'4DNFIPJMZ922.sam.pairs.gz', u'4DNFI45RPI7R.sam.pairs.gz', u'4DNFIWVWWZF9.sam.pairs.gz', u'4DNFIVT1N5YN.sam.pairs.gz', u'4DNFIQP8KWJR.sam.pairs.gz', u'4DNFIFD23W7Z.sam.pairs.gz', u'4DNFICHFDB9J.sam.pairs.gz', u'4DNFIR7BHTXP.sam.pairs.gz', u'4DNFII9KPR27.sam.pairs.gz']}]}\n",
      "about to start run pairsam-merge_4DNEXRAEERUF\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2017, 9, 16, 3, 11, 29, 93000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '429d95c5-9aae-11e7-b4a8-33e2e1185fe8', 'HTTPHeaders': {'x-amzn-requestid': '429d95c5-9aae-11e7-b4a8-33e2e1185fe8', 'content-length': '153', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:pairsam-merge_4DNEXRAEERUF'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:run_awsem_workflow_with_ponies:pairsam-merge_4DNEXRAEERUF\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "import time\n",
    "\n",
    "pairsem_files = exps_pairsems\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "# todo need a function to determin this given fastq1\n",
    "for exp_pairsem_files in pairsem_files:\n",
    "    pairsam1 = make_input_file_json(exp_pairsem_files[1], 'input_pairsams', tibanna,output_file_bucket)\n",
    "    input_files = [pairsam1]\n",
    "    if all(input_files):\n",
    "        name = exp_pairsem_files[0]\n",
    "        input_json = make_hic2b_json(input_files, env, output_file_bucket, name)\n",
    "        print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    time.sleep(5)\n",
    "    #a = raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
