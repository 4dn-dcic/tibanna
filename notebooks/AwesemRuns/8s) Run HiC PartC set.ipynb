{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "#format for input json in hic-partII\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "   \n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    if not isinstance(obj_ids, list):\n",
    "        obj_ids = [ obj_ids ]     \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "         \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])     \n",
    "    if len(uuid_list)==1:\n",
    "        uuid_list = uuid_list[0]\n",
    "    if len(object_key_list)==1:\n",
    "        object_key_list = object_key_list[0]  \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_input_file_json2(obj_ids, arg_name, tibanna, bucket):\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    if not isinstance(obj_ids, list):\n",
    "        obj_ids = [ obj_ids ]     \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "         \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])     \n",
    "#     if len(uuid_list)==1:\n",
    "#         uuid_list = uuid_list[0]\n",
    "#     if len(object_key_list)==1:\n",
    "#         object_key_list = object_key_list[0]  \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_hic8_json(input_files, env, output_bucket, accession):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"c6480905-49e5-4c33-afab-9ec90d65faf3\",\n",
    "                  \"app_name\": \"hi-c-processing-partc\",\n",
    "                  \"parameters\": {\n",
    "                          \"ncores\": 4\n",
    "                      },\n",
    "                  \"tag\": \"set\",\n",
    "                  \"config\": {\n",
    "                        \"ami_id\": \"ami-cfb14bb5\",\n",
    "                        \"json_bucket\": \"4dn-aws-pipeline-run-json\",\n",
    "                        \"ebs_iops\": 5000,\n",
    "                        \"shutdown_min\": 30,\n",
    "                        \"s3_access_arn\": \"arn:aws:iam::643366669028:instance-profile/S3_access\",\n",
    "                        \"ebs_type\": \"io1\",\n",
    "                        \"copy_to_s3\": True,\n",
    "                        \"script_url\": \"https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/\",\n",
    "                        \"key_name\": \"4dn-encode\",\n",
    "                        \"launch_instance\": True,\n",
    "                        \"password\": \"notmypassword\",\n",
    "                        \"log_bucket\": \"tibanna-output\"\n",
    "                      },\n",
    "                  \"_tibanna\": {\"env\": env, \n",
    "                               \"run_type\": \"partc\",\n",
    "                               \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "\n",
    "def get_wfr_out(emb_file, wfr_name, file_format):\n",
    "    workflows = emb_file.get('workflow_run_inputs')\n",
    "    wfr = {}\n",
    "    run_status = 'did not run'\n",
    "    if workflows:\n",
    "        for a_wfr in workflows:\n",
    "            wfr_resp = ff_utils.get_metadata(a_wfr['uuid'], connection=ff)  \n",
    "            wfr_resp_name = wfr_resp['display_title']\n",
    "            if wfr_resp_name.startswith(wfr_name):\n",
    "                wfr = wfr_resp\n",
    "                run_status = wfr_resp['run_status']\n",
    "    else:\n",
    "        return \"no workflow in file\"\n",
    "    \n",
    "    if run_status == 'complete':\n",
    "        outputs = wfr.get('output_files')\n",
    "        file_id = [i['value'] for i in outputs if i['format'] == file_format][0]\n",
    "        if file_id:\n",
    "            return file_id\n",
    "        else:\n",
    "            return \"no file found\"\n",
    "    else:\n",
    "        print wfr_name\n",
    "        return \"no completed run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from invoke import run\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def form_hyp(id):\n",
    "    hyp = '=HYPERLINK(\"https://data.4dnucleome.org/{0}\",\"{0}\")'.format(id)\n",
    "    return hyp\n",
    "\n",
    "\n",
    "all_sets = [\n",
    "            'dcic:Selvaraj_gm12878_hic',\n",
    "            'dekker-lab:ExperimentSet_U54_U54-ESC4DN-FA-DpnII-2017524',\n",
    "            'dekker-lab:ExperimentSet_U54_HFFc6-FA-DpnII'\n",
    "            ]\n",
    "   \n",
    "my_rep_set = all_sets[2]\n",
    "print my_rep_set\n",
    "\n",
    "wf_partI = \"bwa-mem\"\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "enzymes = []\n",
    "\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "rep_set_resp = ff_utils.get_metadata(my_rep_set, connection=ff)\n",
    "rep_resp = rep_set_resp['experiments_in_set']\n",
    "set_acc = rep_set_resp['accession']\n",
    "\n",
    "exps_partC = []\n",
    "all_fine = True\n",
    "\n",
    "f_pairs = 0\n",
    "for exp in rep_resp:    \n",
    "    # print 'Experiment', exp\n",
    "    exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "    enzyme = exp_resp['digestion_enzyme']\n",
    "    enzymes.append(enzyme)\n",
    "    exp_files = exp_resp['files']\n",
    "    exp_acc = exp_resp['accession']\n",
    "    for fastq_file in exp_files:\n",
    "        file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded')  \n",
    "        #Some checks before running\n",
    "        #check if status is deleted\n",
    "        if file_resp['status'] == 'deleted':\n",
    "            print \"delete file\", file_resp['accession']\n",
    "            continue\n",
    "        #if no uploaded file in the file item report and skip\n",
    "        if not file_resp.get('filename'):\n",
    "            print file_resp['accession'], \"does not have a file\"\n",
    "            continue\n",
    "        # check if file is in s3\n",
    "        head_info = tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket)\n",
    "        if not head_info:\n",
    "            print file_resp['accession'], \"does not have a file in S3\"\n",
    "            continue\n",
    "        \n",
    "        # skip pair no 2\n",
    "        if file_resp.get('paired_end')=='2':\n",
    "            continue\n",
    "        f_pairs += 1\n",
    "        paired_file = file_resp['related_files'][0]['file']['accession']\n",
    "        # print file_resp['accession'], paired_file,\n",
    "        \n",
    "        #Check for partI\n",
    "        bam_file = get_wfr_out(file_resp, \"bwa-mem\", 'bam')\n",
    "        if bam_file.startswith('no') or not bam_file:\n",
    "            print bam_file\n",
    "            all_fine = False\n",
    "            continue \n",
    "        else:\n",
    "            bam_resp = ff_utils.get_metadata(bam_file, connection=ff, frame='embedded')\n",
    "            \n",
    "            # Check for part II\n",
    "            pairsem_file = get_wfr_out(bam_resp, \"pairsam-parse-sort\", 'pairsam')\n",
    "            if pairsem_file.startswith('no') or not pairsem_file:\n",
    "                print pairsem_file\n",
    "                all_fine = False\n",
    "                continue\n",
    "            else:\n",
    "                pairsem_resp = ff_utils.get_metadata(pairsem_file, connection=ff, frame='embedded')\n",
    "                \n",
    "                #check for result of part IIB\n",
    "                pairsem_all_file = get_wfr_out(pairsem_resp, \"pairsam-merge\", 'pairsam')\n",
    "                if pairsem_all_file.startswith('no') or not pairsem_all_file:\n",
    "                    print pairsem_all_file\n",
    "                    all_fine = False\n",
    "                    continue\n",
    "                else:\n",
    "                    pairsem_all_resp = ff_utils.get_metadata(pairsem_all_file, connection=ff, frame='embedded')\n",
    "                    \n",
    "                    #check for result of part III\n",
    "                    pairsem_md = get_wfr_out(pairsem_all_resp, \"pairsam-markasdup\", 'pairsam')\n",
    "                    if pairsem_md.startswith('no') or not pairsem_md:\n",
    "                        print pairsem_md\n",
    "                        all_fine = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        pairsem_md_resp = ff_utils.get_metadata(pairsem_md, connection=ff, frame='embedded')\n",
    "                        \n",
    "                        \n",
    "                        #check for result of part IV\n",
    "                        pairsem_ft = get_wfr_out(pairsem_md_resp, \"pairsam-filter\", 'pairs')\n",
    "                        if pairsem_ft.startswith('no') or not pairsem_ft:\n",
    "                            print pairsem_ft\n",
    "                            all_fine = False\n",
    "                            continue\n",
    "                        else:\n",
    "                            pairsem_ft_resp = ff_utils.get_metadata(pairsem_ft, connection=ff, frame='embedded')\n",
    "                            \n",
    "                            #check for result of part V\n",
    "                            fr_pairs = get_wfr_out(pairsem_ft_resp, \"addfragtopairs\", 'pairs')\n",
    "                            if fr_pairs.startswith('no') or not fr_pairs:\n",
    "                                print fr_pairs\n",
    "                                all_fine = False\n",
    "                                continue\n",
    "                            else:\n",
    "                                fr_pairs_resp = ff_utils.get_metadata(fr_pairs, connection=ff, frame='embedded')\n",
    "                            \n",
    "                                #check for result of part VI\n",
    "                                pt_pairs = get_wfr_out(fr_pairs_resp, \"pairs-patch\", 'pairs')\n",
    "                                if pt_pairs.startswith('no') or not pt_pairs:\n",
    "                                    print pt_pairs\n",
    "                                    all_fine = False\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    pt_pairs_resp = ff_utils.get_metadata(pt_pairs, connection=ff, frame='embedded')\n",
    "                                    \n",
    "                                    #check for result of partB before partC\n",
    "                                    cool_f = get_wfr_out(pt_pairs_resp, \"hi-c-processing-partb set\", 'cool')\n",
    "                                    hic_f = get_wfr_out(pt_pairs_resp, \"hi-c-processing-partb set\", 'hic')\n",
    "                                    if cool_f.startswith('no') or not cool_f:\n",
    "                                        print cool_f\n",
    "                                        all_fine = False\n",
    "                                        continue\n",
    "                                    elif hic_f.startswith('no') or not hic_f:\n",
    "                                        print hic_f\n",
    "                                        all_fine = False\n",
    "                                        continue  \n",
    "                                    else:\n",
    "                                        cool_resp = ff_utils.get_metadata(cool_f, connection=ff, frame='embedded')\n",
    "                                        hic_resp = ff_utils.get_metadata(hic_f, connection=ff, frame='embedded')\n",
    "\n",
    "\n",
    "                                        f_s1 = round(cool_resp['file_size']/(1024*1024*1024.0),2)\n",
    "                                        f_s2 = round(hic_resp['file_size']/(1024*1024*1024.0),2)\n",
    "                                        print \"cool\"\n",
    "                                        print form_hyp(cool_resp[\"accession\"])+'\\t'+cool_resp[\"uuid\"]+\"\\t\"+str(f_s1)\n",
    "                                        print \"hic\"\n",
    "                                        print form_hyp(hic_resp[\"accession\"])+'\\t'+hic_resp[\"uuid\"]+\"\\t\"+str(f_s2)\n",
    "                        \n",
    "        exps_partC.append((exp_acc,cool_resp[\"accession\"],hic_resp[\"accession\"]))\n",
    "        break\n",
    "\n",
    "print exps_partC\n",
    "\n",
    "cools = list(set([i[1] for i in exps_partC]))\n",
    "hics = list(set([i[2] for i in exps_partC]))\n",
    "if len(cools) != 1 or len(hics) != 1:\n",
    "    print \"Inconsistent results from different experiments of same set\"\n",
    "\n",
    "exps_partC = [set_acc, cools[0], hics[0]]\n",
    "    \n",
    "print exps_partC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "import time\n",
    "\n",
    "partC_files = [exps_partC]\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "chrsizes = make_input_file_json('4DNFI823LSII', 'chromsize', tibanna, raw_file_bucket)\n",
    "#restrict = make_input_file_json(re_ref_file, 'restriction_file', tibanna, raw_file_bucket)\n",
    "\n",
    "# todo need a function to determin this given fastq1\n",
    "for exp_acc, cool_id, hic_id in partC_files:\n",
    "    cool_input = make_input_file_json(cool_id, 'input_cool', tibanna, output_file_bucket)\n",
    "    hic_input = make_input_file_json(hic_id, 'input_hic', tibanna, output_file_bucket)\n",
    "    input_files = [cool_input, hic_input, chrsizes]\n",
    "    if all(input_files):\n",
    "        name = exp_acc\n",
    "        input_json = make_hic8_json(input_files, env, output_file_bucket, name)\n",
    "        print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    time.sleep(5)\n",
    "    #a = raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
