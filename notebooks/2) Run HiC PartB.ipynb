{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "wf_partA = \"hi-c-processing-parta-juicer/\"\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "\n",
    "\n",
    "def summarize_file(file_resp):\n",
    "    wf_partA = \"hi-c-processing-parta-juicer/\"\n",
    "    file_id = file_resp['accession']\n",
    "    if file_id in ['4DNFICFKYPEG','4DNFI75B5A1N']:\n",
    "        wf_partA= \"hi-c-processing-parta-juicer/17\"\n",
    "    relations = file_resp.get('related_files')\n",
    "    workflows = file_resp.get('workflow_run_inputs')\n",
    "    first_alias = file_resp.get('aliases',[None])[0]\n",
    "    # get related file\n",
    "    paired_file = ''\n",
    "    for relation in relations:\n",
    "        if relation['relationship_type'] == 'paired with':\n",
    "            paired_file = relation['file']['accession']\n",
    "    # Check workflows workflow partA\n",
    "    last_part_A = ''\n",
    "    last_part_A_status = 'did_not_run'\n",
    "    # Assumes workflow_runs come in time ordered list, and grabs the last ones for each wf run\n",
    "    if workflows:\n",
    "        for wfr_resp in workflows:\n",
    "            wfr_name = wfr_resp['display_title']   \n",
    "            if wfr_name.startswith(wf_partA):\n",
    "                last_part_A=wfr_resp['uuid']\n",
    "                last_part_A_status = wfr_resp.get('run_status')  \n",
    "    # return a small report\n",
    "    return {'file': file_id,\n",
    "            'alias': first_alias,\n",
    "            'paired_file': paired_file,\n",
    "            'last_part_A': last_part_A,\n",
    "            'last_part_A_status': last_part_A_status\n",
    "           }\n",
    "\n",
    "#changed worflow uuid\n",
    "# it was b9829418-49e5-4c33-afab-9ec90d65faf3\n",
    "# new one is b9829418-49e5-4c33-afab-9ec90d659999\n",
    "def make_hicb_json(input_files, env, output_bucket, accession, ncores):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"b9829418-49e5-4c33-afab-9ec90d659999\",\n",
    "                  \"app_name\": \"hi-c-processing-partb/26\",\n",
    "                  \"parameters\": {\n",
    "                      \"ncores\" :  ncores,\n",
    "                      \"binsize\": 5000,\n",
    "                      \"min_res\": 5000\n",
    "                  },\n",
    "                  \"_tibanna\": {\"env\": env, \"run_type\": \"hic-partb\", \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    input_is_array = True\n",
    "    if not isinstance(obj_ids, list):\n",
    "        input_is_array = False\n",
    "        obj_ids = [ obj_ids ]\n",
    "        \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "     \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            print tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket)\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])\n",
    "            \n",
    "    if not input_is_array: \n",
    "        uuid_list = uuid_list[0]\n",
    "        object_key_list = object_key_list[0]\n",
    "        \n",
    "    data = {'bucket_name' : bucket,\n",
    "                    'object_key' :  object_key_list,\n",
    "                    'uuid' : uuid_list,\n",
    "                    'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    input_is_array = True\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    \n",
    "    if not isinstance(obj_ids, list):\n",
    "        input_is_array = False\n",
    "        obj_ids = [ obj_ids ]\n",
    "        \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "     \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])\n",
    "            \n",
    "    if not input_is_array:\n",
    "        uuid_list = uuid_list[0]\n",
    "        object_key_list = object_key_list[0]\n",
    "        \n",
    "    data = {'bucket_name' : bucket,\n",
    "                    'object_key' :  object_key_list,\n",
    "                    'uuid' : uuid_list,\n",
    "                    'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report on dciclab:rao_rep07\n",
      "/files-processed/4DNFI6S86A2O/ 3.2 (corrected)\n",
      "/files-processed/4DNFIPPJT80S/ 3.1 (corrected)\n",
      "/files-processed/4DNFITRG12ND/ 3.1 (corrected)\n",
      "/files-processed/4DNFI9P1LZXW/ 3.1 (corrected)\n",
      "/files-processed/4DNFI1UK8DQI/ 3.1 (corrected)\n",
      "/files-processed/4DNFIVDZPOMT/ 3.1 (corrected)\n",
      "/files-processed/4DNFIMGJAJH9/ 2.8 (corrected)\n",
      "/files-processed/4DNFI6VCZL2S/ 2.8 (corrected)\n",
      "/files-processed/4DNFIZ3VW0QY/ 2.9 (corrected)\n",
      "/files-processed/4DNFI58GABL4/ 3.2 (corrected)\n",
      "/files-processed/4DNFI687CALS/ 3.1 (corrected)\n",
      "/files-processed/4DNFIREOP46C/ 3.1 (corrected)\n",
      "/files-processed/4DNFIJUH6R5X/ 3.2 (corrected)\n",
      "/files-processed/4DNFI8HY9EW2/ 3.2 (corrected)\n",
      "/files-processed/4DNFIM9ZBJ6B/ 3.2 (corrected)\n",
      "/files-processed/4DNFIMUQ4JGB/ 3.1 (corrected)\n",
      "/files-processed/4DNFI7SAFUTN/ 3.1 (corrected)\n",
      "/files-processed/4DNFIHG8M8IY/ 2.9 (corrected)\n",
      "/files-processed/4DNFIWOI72SY/ 2.9 (corrected)\n",
      "/files-processed/4DNFI780HHYN/ 2.9 (corrected)\n",
      "/files-processed/4DNFI46L2PZ4/ 2.9 (corrected)\n",
      "/files-processed/4DNFI1M50Z3F/ 2.9 (corrected)\n",
      "/files-processed/4DNFIY0L2Q6D/ 2.8 (corrected)\n",
      "/files-processed/4DNFIHVW6S99/ 2.9 (corrected)\n",
      "/files-processed/4DNFIYAF36AY/ 2.6 (corrected)\n",
      "/files-processed/4DNFIC9R9ME7/ 2.5 (corrected)\n",
      "/files-processed/4DNFIQJZ5FT7/ 2.7 (corrected)\n",
      "/files-processed/4DNFIY3OF737/ 2.7 (corrected)\n",
      "/files-processed/4DNFIML7JR7J/ 2.6 (corrected)\n",
      "/files-processed/4DNFIGLNZ5NI/ 2.6 (corrected)\n",
      "/files-processed/4DNFI3HG26I1/ 2.5 (corrected)\n",
      "/files-processed/4DNFI9M3TUFF/ 17.9 (corrected)\n",
      "108.7GB total file size\n",
      "\n",
      "report on dciclab:rao_rep02\n",
      "/files-processed/4DNFIKB23R9H/ 9.4\n",
      "/files-processed/4DNFIY0QMTA8/ 3.2\n",
      "/files-processed/4DNFI8OYLP2T/ 1.6\n",
      "/files-processed/4DNFI1C97MZ6/ 2.3\n",
      "/files-processed/4DNFIUA41YZK/ 2.3\n",
      "/files-processed/4DNFIW1I1LI6/ 3.1\n",
      "/files-processed/4DNFIXV3ACPK/ 3.6\n",
      "/files-processed/4DNFIFAAMR3G/ 4.1\n",
      "/files-processed/4DNFI0U1SFJJ/ 2.4\n",
      "/files-processed/4DNFIO9EV5ME/ 1.2\n",
      "/files-processed/4DNFIPZZNRT5/ 1.2\n",
      "/files-processed/4DNFIJRGTQ5W/ 3.7\n",
      "/files-processed/4DNFI2GQP8Y3/ 1.6\n",
      "/files-processed/4DNFI3OEFAAG/ 3.0\n",
      "/files-processed/4DNFIDURNGL2/ 2.8\n",
      "/files-processed/4DNFIJ21CZZX/ 2.6\n",
      "/files-processed/4DNFIQA4L9ME/ 2.6\n",
      "/files-processed/4DNFIID81EW1/ 3.1\n",
      "/files-processed/4DNFI1YK5FUF/ 2.6\n",
      "/files-processed/4DNFITB1KEMI/ 3.5\n",
      "/files-processed/4DNFIBCK8219/ 3.3\n",
      "/files-processed/4DNFI1SHSZ7Z/ 3.3\n",
      "/files-processed/4DNFI6NDND1Y/ 3.6\n",
      "/files-processed/4DNFI8BYK530/ 2.6\n",
      "/files-processed/4DNFIQMS9CMO/ 2.2\n",
      "/files-processed/4DNFIQ55AP58/ 7.0\n",
      "/files-processed/4DNFITST9ZES/ 7.2\n",
      "/files-processed/4DNFIIZ27RVI/ 6.4\n",
      "/files-processed/4DNFIQS0I93Y/ 1.5\n",
      "/files-processed/4DNFIDHSGODS/ 3.3\n",
      "/files-processed/4DNFINWSUWBX/ 2.0\n",
      "/files-processed/4DNFIVJUFPQ2/ 2.2\n",
      "104.5GB total file size\n",
      "\n",
      "report on dcic:Jin_imr90_hic\n",
      "/files-processed/4DNFI86U73RR/ 0.8 (corrected)\n",
      "/files-processed/4DNFIH2OGFY7/ 0.7 (corrected)\n",
      "/files-processed/4DNFIB48N3K0/ 0.8 (corrected)\n",
      "/files-processed/4DNFIM3IWJ6D/ 0.8 (corrected)\n",
      "/files-processed/4DNFI4AH08KA/ 1.9 (corrected)\n",
      "/files-processed/4DNFI9ZPT3J8/ 2.9 (corrected)\n",
      "/files-processed/4DNFI47EYMI6/ 1.1 (corrected)\n",
      "/files-processed/4DNFI0AJW9U7/ 1.1 (corrected)\n",
      "/files-processed/4DNFIUJ9K3PP/ 1.1 (corrected)\n",
      "/files-processed/4DNFIXS50TEK/ 3.2 (corrected)\n",
      "/files-processed/4DNFI0XS1ZA7/ 1.7 (corrected)\n",
      "/files-processed/4DNFIBPXNSVQ/ 3.3 (corrected)\n",
      "/files-processed/4DNFIFGEVFEF/ 2.2 (corrected)\n",
      "/files-processed/4DNFID8IALLW/ 2.2 (corrected)\n",
      "/files-processed/4DNFICD6PHVJ/ 2.2 (corrected)\n",
      "/files-processed/4DNFI8O3N8T0/ 3.1 (corrected)\n",
      "/files-processed/4DNFIGNMVV55/ 2.6 (corrected)\n",
      "31.7GB total file size\n",
      "\n",
      "no of pairs sets\n",
      "3\n",
      "[['dciclab:rao_rep07', [u'/files-processed/4DNFI6S86A2O/', u'/files-processed/4DNFIPPJT80S/', u'/files-processed/4DNFITRG12ND/', u'/files-processed/4DNFI9P1LZXW/', u'/files-processed/4DNFI1UK8DQI/', u'/files-processed/4DNFIVDZPOMT/', u'/files-processed/4DNFIMGJAJH9/', u'/files-processed/4DNFI6VCZL2S/', u'/files-processed/4DNFIZ3VW0QY/', u'/files-processed/4DNFI58GABL4/', u'/files-processed/4DNFI687CALS/', u'/files-processed/4DNFIREOP46C/', u'/files-processed/4DNFIJUH6R5X/', u'/files-processed/4DNFI8HY9EW2/', u'/files-processed/4DNFIM9ZBJ6B/', u'/files-processed/4DNFIMUQ4JGB/', u'/files-processed/4DNFI7SAFUTN/', u'/files-processed/4DNFIHG8M8IY/', u'/files-processed/4DNFIWOI72SY/', u'/files-processed/4DNFI780HHYN/', u'/files-processed/4DNFI46L2PZ4/', u'/files-processed/4DNFI1M50Z3F/', u'/files-processed/4DNFIY0L2Q6D/', u'/files-processed/4DNFIHVW6S99/', u'/files-processed/4DNFIYAF36AY/', u'/files-processed/4DNFIC9R9ME7/', u'/files-processed/4DNFIQJZ5FT7/', u'/files-processed/4DNFIY3OF737/', u'/files-processed/4DNFIML7JR7J/', u'/files-processed/4DNFIGLNZ5NI/', u'/files-processed/4DNFI3HG26I1/', u'/files-processed/4DNFI9M3TUFF/']], ['dciclab:rao_rep02', [u'/files-processed/4DNFIKB23R9H/', u'/files-processed/4DNFIY0QMTA8/', u'/files-processed/4DNFI8OYLP2T/', u'/files-processed/4DNFI1C97MZ6/', u'/files-processed/4DNFIUA41YZK/', u'/files-processed/4DNFIW1I1LI6/', u'/files-processed/4DNFIXV3ACPK/', u'/files-processed/4DNFIFAAMR3G/', u'/files-processed/4DNFI0U1SFJJ/', u'/files-processed/4DNFIO9EV5ME/', u'/files-processed/4DNFIPZZNRT5/', u'/files-processed/4DNFIJRGTQ5W/', u'/files-processed/4DNFI2GQP8Y3/', u'/files-processed/4DNFI3OEFAAG/', u'/files-processed/4DNFIDURNGL2/', u'/files-processed/4DNFIJ21CZZX/', u'/files-processed/4DNFIQA4L9ME/', u'/files-processed/4DNFIID81EW1/', u'/files-processed/4DNFI1YK5FUF/', u'/files-processed/4DNFITB1KEMI/', u'/files-processed/4DNFIBCK8219/', u'/files-processed/4DNFI1SHSZ7Z/', u'/files-processed/4DNFI6NDND1Y/', u'/files-processed/4DNFI8BYK530/', u'/files-processed/4DNFIQMS9CMO/', u'/files-processed/4DNFIQ55AP58/', u'/files-processed/4DNFITST9ZES/', u'/files-processed/4DNFIIZ27RVI/', u'/files-processed/4DNFIQS0I93Y/', u'/files-processed/4DNFIDHSGODS/', u'/files-processed/4DNFINWSUWBX/', u'/files-processed/4DNFIVJUFPQ2/']], ['dcic:Jin_imr90_hic', [u'/files-processed/4DNFI86U73RR/', u'/files-processed/4DNFIH2OGFY7/', u'/files-processed/4DNFIB48N3K0/', u'/files-processed/4DNFIM3IWJ6D/', u'/files-processed/4DNFI4AH08KA/', u'/files-processed/4DNFI9ZPT3J8/', u'/files-processed/4DNFI47EYMI6/', u'/files-processed/4DNFI0AJW9U7/', u'/files-processed/4DNFIUJ9K3PP/', u'/files-processed/4DNFIXS50TEK/', u'/files-processed/4DNFI0XS1ZA7/', u'/files-processed/4DNFIBPXNSVQ/', u'/files-processed/4DNFIFGEVFEF/', u'/files-processed/4DNFID8IALLW/', u'/files-processed/4DNFICD6PHVJ/', u'/files-processed/4DNFI8O3N8T0/', u'/files-processed/4DNFIGNMVV55/']]]\n",
      "use 'all_pairs' to pass the list of pairs files of each set\n"
     ]
    }
   ],
   "source": [
    "mbo_sets = [\n",
    "'dciclab:rao_rep02',\n",
    "'dciclab:rao_rep12'\n",
    "]\n",
    "\n",
    "\n",
    "# 'dciclab:rao_rep07',\n",
    "#  'dciclab:rao_rep12',\n",
    "all_sets = [ 'dciclab:rao_rep07',\n",
    "            'dciclab:rao_rep02',\n",
    "            'dcic:Jin_imr90_hic'\n",
    "            ]\n",
    "\n",
    "done_sets = [\n",
    "            'dciclab:rao_rep13',\n",
    "            'dcic:Selvaraj_gm12878_hic'\n",
    "            ]\n",
    "\n",
    "all_pairs = []\n",
    "for a_set in all_sets:\n",
    "    print 'report on', a_set\n",
    "    rep_resp = ff_utils.get_metadata(a_set, connection=ff)['experiments_in_set']\n",
    "    total = 0\n",
    "    set_pairs =[]\n",
    "    set_pairs_fine = True  \n",
    "    for exp in rep_resp:\n",
    "        # print 'Experiment', exp\n",
    "        exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "        exp_files = exp_resp['files']\n",
    "        for fastq_file in exp_files:\n",
    "            file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded') \n",
    "            # skip unfortunate status\n",
    "            if file_resp['status'] in ['deleted', 'uploading', 'upload failed']:\n",
    "                continue\n",
    "            # if no uploaded file in the file item report and skip\n",
    "            if not file_resp.get('filename'):\n",
    "                print file_resp['accession'], \"does not have a file\"\n",
    "                continue\n",
    "            # check if file is in s3\n",
    "            if not tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket):\n",
    "                print file_resp['accession'], \"does not have a file in S3\"\n",
    "                continue \n",
    "            # skip miseq\n",
    "            if file_resp.get('instrument') == \"Illumina MiSeq\":\n",
    "                continue\n",
    "            # skip pair no 2\n",
    "            if file_resp.get('paired_end')=='2':\n",
    "                continue \n",
    " \n",
    "            # get report\n",
    "            file_info = summarize_file(file_resp)\n",
    "            # get report on paired file\n",
    "            paired_file = file_info['paired_file']\n",
    "            pair_file_resp = ff_utils.get_metadata(paired_file, connection=ff, frame='embedded')\n",
    "            pair_file_info = summarize_file(pair_file_resp)\n",
    "            \n",
    "            partAfine = False\n",
    "            # check the partA for both paired fastq\n",
    "            if file_info['last_part_A_status'] == 'complete':\n",
    "                if pair_file_info['last_part_A_status'] == 'complete':\n",
    "                    if file_info['last_part_A'] == pair_file_info['last_part_A']:\n",
    "                        partAfine = True\n",
    "                        \n",
    "            ########\n",
    "            #######\n",
    "            #  We need to add a check if partV run before, so don't run twice\n",
    "            #####\n",
    "            ####\n",
    "            ###\n",
    "            ##\n",
    "            #\n",
    "            \n",
    "            hindIII_correction = False\n",
    "            if not partAfine:\n",
    "                set_pairs_fine = False\n",
    "                print file_info['file'], pair_file_info['file'], \"has problems with partA\"\n",
    "                print \"this experiment set will not be part of all_pairs list\"\n",
    "            else:\n",
    "                partA_data = ff_utils.get_metadata(file_info['last_part_A'], connection=ff)\n",
    "                inputs = partA_data.get('input_files')\n",
    "                # An old version of the hindIII restriction file needed to be corrected\n",
    "                # If found that as the input file, look at the wfr on pair file and use the \n",
    "                # correct one\n",
    "                nz = [i['value'] for i in inputs if i['workflow_argument_name']=='restriction_file'][0]\n",
    "                if nz == '/files-reference/4DNFI823L811/':\n",
    "                    hindIII_correction = True\n",
    "                outputs = partA_data.get('output_files')\n",
    "                pair_file = [i['value'] for i in outputs if i['format'] == 'pairs'][0]\n",
    "                pair_resp = ff_utils.get_metadata(pair_file, connection=ff, frame='embedded')  \n",
    "                \n",
    "                # check if file is in s3\n",
    "                head_info = tibanna.s3.does_key_exist(pair_resp['upload_key'], tibanna.s3.outfile_bucket)\n",
    "                if not head_info:\n",
    "                    set_pairs_fine = False\n",
    "                    print pair_resp['accession'], \"does not have a file in S3, skipping this set\"\n",
    "                    continue\n",
    "                \n",
    "                ### if there was a correction for partA, find the right pairs file\n",
    "                if hindIII_correction:\n",
    "                    pair_wfrs = pair_resp['workflow_run_inputs']\n",
    "                    if pair_wfrs:\n",
    "                        for wfr_resp in pair_wfrs:\n",
    "                            wfr_name = wfr_resp['display_title']   \n",
    "                            if wfr_name.startswith('hi-c-processing-parta-juicer-patch'):\n",
    "                                last_part_A_cor=wfr_resp['uuid']\n",
    "                                last_part_A_cor_status = wfr_resp.get('run_status') \n",
    "                    if last_part_A_cor_status != 'complete':\n",
    "                        set_pairs_fine = False\n",
    "                        print \"There is a problem with partA_correction run for pair\", pair_resp['accession']\n",
    "                        continue\n",
    "                    else:\n",
    "                        cor_wfr = ff_utils.get_metadata(last_part_A_cor, connection=ff)\n",
    "                        cor_outputs = cor_wfr.get('output_files')\n",
    "                        cor_pair_file = [i['value'] for i in cor_outputs if i['format'] == 'pairs'][0]\n",
    "                        cor_pair_resp = ff_utils.get_metadata(cor_pair_file, connection=ff)  \n",
    "                        # check if file is in s3\n",
    "                        cor_head_info = tibanna.s3.does_key_exist(cor_pair_resp['upload_key'], tibanna.s3.outfile_bucket)\n",
    "                        if not cor_head_info:\n",
    "                            set_pairs_fine = False\n",
    "                            print pair_resp['accession'], \"does not have the corrected pair file\", cor_pair_resp['accession'] ,\"in S3, skipping this set\"\n",
    "                            continue\n",
    "                        \n",
    "                        cor_file_size = round(cor_head_info['ContentLength']/1073741824.0,1)\n",
    "                        total += cor_file_size\n",
    "                        print cor_pair_file, str(cor_file_size), \"(corrected)\"\n",
    "                        set_pairs.append(cor_pair_file)\n",
    "                else:\n",
    "                    file_size = round(head_info['ContentLength']/1073741824.0,1)\n",
    "                    total += file_size\n",
    "                    print pair_file, str(file_size)\n",
    "                    set_pairs.append(pair_file)\n",
    "\n",
    "\n",
    "                \n",
    "    if set_pairs_fine:\n",
    "        all_pairs.append([a_set, set_pairs])\n",
    "    print str(total) + \"GB total file size\"\n",
    "    print\n",
    "\n",
    "print \"no of pairs sets\"\n",
    "print len(all_pairs)\n",
    "print all_pairs\n",
    "print \"use 'all_pairs' to pass the list of pairs files of each set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "from core import ff_utils\n",
    "import time\n",
    "\n",
    "# testportal\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "test_pairs = [\n",
    "    ['small_set_1.2gb_1.2gb', ['/files-processed/4DNFIO9EV5ME/', '/files-processed/4DNFIPZZNRT5/']],\n",
    "    ['medium_set_2.3gb_2.4gb',['/files-processed/4DNFI0U1SFJJ/', '/files-processed/4DNFI1C97MZ6/']],\n",
    "    ['large_set_3.6gb_3.6gb',['/files-processed/4DNFIXV3ACPK/', '/files-processed/4DNFI6NDND1Y/']]\n",
    "]\n",
    "\n",
    "\n",
    "for set_name, pair_list in all_pairs:\n",
    "    print set_name\n",
    "    print pair_list\n",
    "    chrsizes = make_input_file_json('4DNFI823LSII', 'chrsizes', tibanna, raw_file_bucket)\n",
    "    pair_files= make_input_file_json(pair_list, 'input_pairs', tibanna, output_file_bucket)\n",
    "    ncores = 32\n",
    "\n",
    "    # ncore options 8  cores up to 20gb per .fastq.gz (1tb)\n",
    "    #               36 cores up to 42gb per .fastq.gz (2tb)\n",
    "    #     Not set            up to 90gb per .fastq.gz (4tb)\n",
    "\n",
    "    input_files = [chrsizes, pair_files]\n",
    "    if all(input_files):\n",
    "        name = 'partB_'+ set_name.replace(\":\", \"_\")\n",
    "        input_json = make_hicb_json(input_files, env, output_file_bucket, name, ncores)\n",
    "        # print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    time.sleep(1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
