{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "from datetime import datetime\n",
    "\n",
    "wf_partA = \"hi-c-processing-parta-juicer/25\"\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "\n",
    "\n",
    "def summarize_file(file_resp):\n",
    "    file_id = file_resp['accession']\n",
    "    wf_partA = \"hi-c-processing-parta-juicer/25\"\n",
    "    relations = file_resp.get('related_files')\n",
    "    workflows = file_resp.get('workflow_run_inputs')\n",
    "    first_alias = file_resp.get('aliases',[None])[0]\n",
    "    # get related file\n",
    "    paired_file = ''\n",
    "    for relation in relations:\n",
    "        if relation['relationship_type'] == 'paired with':\n",
    "            paired_file = relation['file']['accession']\n",
    "    # Check workflows workflow partA\n",
    "    last_part_A = ''\n",
    "    last_part_A_status = 'did_not_run'\n",
    "    \n",
    "    # Assumes workflow_runs come in time ordered list, and grabs the last ones for each wf run\n",
    "    wfr_report = []\n",
    "    if workflows:\n",
    "        for wfr_resp in workflows:\n",
    "            wfr_report.append(get_wfr_report(wfr_resp))\n",
    "        wfr_report = sorted(wfr_report, key=lambda k: (k['wfr_date'], k['wfr_name']))   \n",
    "    if wfr_report:\n",
    "        for report in wfr_report:\n",
    "            if report['wfr_name'].startswith(wf_partA):\n",
    "                last_part_A = report.get('wfr_uuid')\n",
    "                last_part_A_status = report.get('wfr_status') \n",
    "        \n",
    "    # return a small report\n",
    "    return {'file': file_id,\n",
    "            'alias': first_alias,\n",
    "            'paired_file': paired_file,\n",
    "            'last_part_A': last_part_A,\n",
    "            'last_part_A_status': last_part_A_status\n",
    "           }\n",
    "\n",
    "\n",
    "def get_wfr_report(wfr_data):\n",
    "    \"\"\"For a given workflow_run_sbg item, grabs details, uuid, run_status, wfr name, date, and run time\"\"\"\n",
    "    wfr_uuid = wfr_data['uuid']\n",
    "    wfr_status = wfr_data['run_status']\n",
    "    wfr_name = wfr_data['title'].split(' run ')[0]\n",
    "    wfr_time = datetime.strptime(wfr_data['date_created'],'%Y-%m-%dT%H:%M:%S.%f+00:00')\n",
    "    run_hours = (datetime.now()-wfr_time).total_seconds()/3600\n",
    "    wfr_rep = {'wfr_uuid': wfr_data['uuid'],\n",
    "               'wfr_status': wfr_data['run_status'],\n",
    "               'wfr_name': wfr_data['title'].split(' run ')[0],\n",
    "               'wfr_date': wfr_time,\n",
    "               'run_time': run_hours}\n",
    "    return wfr_rep\n",
    "\n",
    "\n",
    "def make_hicb_json(input_files, env, output_bucket, accession, ncores):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"b9829418-49e5-4c33-afab-9ec90d659999\",\n",
    "                  \"app_name\": \"hi-c-processing-partb/28\",\n",
    "                  \"parameters\": {\n",
    "                      \"ncores\" : ncores,\n",
    "                      \"binsize\": 5000,\n",
    "                      \"min_res\": 5000\n",
    "                  },\n",
    "                  \"_tibanna\": {\"env\": env, \"run_type\": \"hic_part_b\", \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''obj_ids can be either a string or a list.\n",
    "    {\"bucket_name\": \"%s\", \"object_key\": \"%s\", \"uuid\" : \"%s\", \"workflow_argument_name\": \"%s\"}'''\n",
    "    input_is_array = True\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    if not isinstance(obj_ids, list):\n",
    "        input_is_array = False\n",
    "        obj_ids = [ obj_ids ]     \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])         \n",
    "    if not input_is_array:\n",
    "        uuid_list = uuid_list[0]\n",
    "        object_key_list = object_key_list[0]     \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sets = [\n",
    "'dciclab:rao_rep02',\n",
    "'dciclab:rao_rep07',\n",
    "'dciclab:rao_rep12',\n",
    "'dciclab:rao_rep13',\n",
    "'dcic:Selvaraj_gm12878_hic',\n",
    "'dcic:Jin_imr90_hic'\n",
    "]\n",
    "\n",
    "all_pairs = []\n",
    "for a_set in all_sets:\n",
    "    print 'report on', a_set\n",
    "    rep_resp = ff_utils.get_metadata(a_set, connection=ff)['experiments_in_set']\n",
    "    total = 0\n",
    "    set_pairs =[]\n",
    "    set_pairs_fine = True  \n",
    "    for exp in rep_resp:\n",
    "        # print 'Experiment', exp\n",
    "        exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "        exp_files = exp_resp['files']\n",
    "        for fastq_file in exp_files:\n",
    "            file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded') \n",
    "            # skip unfortunate status\n",
    "            if file_resp['status'] in ['deleted', 'uploading', 'upload failed']:\n",
    "                continue\n",
    "            # if no uploaded file in the file item report and skip\n",
    "            if not file_resp.get('filename'):\n",
    "                print file_resp['accession'], \"does not have a file\"\n",
    "                continue\n",
    "            # check if file is in s3\n",
    "            if not tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket):\n",
    "                print file_resp['accession'], \"does not have a file in S3\"\n",
    "                continue \n",
    "            # skip miseq\n",
    "            if file_resp.get('instrument') == \"Illumina MiSeq\":\n",
    "                continue\n",
    "            # skip pair no 2\n",
    "            if file_resp.get('paired_end')=='2':\n",
    "                continue \n",
    " \n",
    "            # get report\n",
    "            file_info = summarize_file(file_resp)\n",
    "            # get report on paired file\n",
    "            paired_file = file_info['paired_file']\n",
    "            pair_file_resp = ff_utils.get_metadata(paired_file, connection=ff, frame='embedded')\n",
    "            pair_file_info = summarize_file(pair_file_resp)\n",
    "            \n",
    "            partAfine = False\n",
    "            # check the partA for both paired fastq\n",
    "            if file_info['last_part_A_status'] == 'complete':\n",
    "                if pair_file_info['last_part_A_status'] == 'complete':\n",
    "                    if file_info['last_part_A'] == pair_file_info['last_part_A']:\n",
    "                        partAfine = True\n",
    "                        \n",
    "            ########\n",
    "            #######\n",
    "            #  We need to add a check if partV run before, so don't run twice\n",
    "            #####\n",
    "            ####\n",
    "            ###\n",
    "            ##\n",
    "            #\n",
    "          \n",
    "            if not partAfine:\n",
    "                set_pairs_fine = False\n",
    "                print file_info['file'], pair_file_info['file'], \"has problems with partA\"\n",
    "                print \"this experiment set will not be part of all_pairs list\"\n",
    "                \n",
    "            # if partA is fine\n",
    "            else:\n",
    "                partA_data = ff_utils.get_metadata(file_info['last_part_A'], connection=ff)\n",
    "                inputs = partA_data.get('input_files')\n",
    "                    \n",
    "                    \n",
    "                outputs = partA_data.get('output_files')\n",
    "                pair_file = [i['value'] for i in outputs if i['format'] == 'pairs'][0]\n",
    "                pair_resp = ff_utils.get_metadata(pair_file, connection=ff, frame='embedded')  \n",
    "                \n",
    "                # check if file is in s3\n",
    "                head_info = tibanna.s3.does_key_exist(pair_resp['upload_key'], tibanna.s3.outfile_bucket)\n",
    "                if not head_info:\n",
    "                    set_pairs_fine = False\n",
    "                    print pair_resp['accession'], \"does not have a file in S3, skipping this set\"\n",
    "                    continue\n",
    "                file_size = round(head_info['ContentLength']/1073741824.0,1)\n",
    "                total += file_size\n",
    "                print pair_file, str(file_size)\n",
    "                set_pairs.append(pair_file)\n",
    "    \n",
    "    if set_pairs_fine:\n",
    "        all_pairs.append([a_set, set_pairs])\n",
    "    print str(total) + \"GB total file size\"\n",
    "    print\n",
    "\n",
    "print \"no of pairs sets\"\n",
    "print len(all_pairs)\n",
    "print \"use 'all_pairs' to pass the list of pairs files of each set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "from core import ff_utils\n",
    "import time\n",
    "\n",
    "# testportal\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "test_pairs = [\n",
    "    ['small_set_1.2gb_1.2gb', ['/files-processed/4DNFIO9EV5ME/', '/files-processed/4DNFIPZZNRT5/']],\n",
    "    ['medium_set_2.3gb_2.4gb',['/files-processed/4DNFI0U1SFJJ/', '/files-processed/4DNFI1C97MZ6/']],\n",
    "    ['large_set_3.6gb_3.6gb',['/files-processed/4DNFIXV3ACPK/', '/files-processed/4DNFI6NDND1Y/']]\n",
    "]\n",
    "\n",
    "for set_name, pair_list in all_pairs:\n",
    "    chrsizes = make_input_file_json('4DNFI823LSII', 'chrsizes', tibanna, raw_file_bucket)\n",
    "    pair_files= make_input_file_json(pair_list, 'input_pairs', tibanna, output_file_bucket)\n",
    "    ncores = 32\n",
    "\n",
    "    input_files = [chrsizes, pair_files]\n",
    "    if all(input_files):\n",
    "        name = set_name.replace(\":\", \"_\")\n",
    "        input_json = make_hicb_json(input_files, env, output_file_bucket, name, ncores)\n",
    "        res = run_workflow(input_json)\n",
    "        print\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "        print\n",
    "    time.sleep(10)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
