{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "wf_partA = \"hi-c-processing-parta-juicer/\"\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "\n",
    "\n",
    "def summarize_file(file_resp):\n",
    "    file_id = file_resp['accession']\n",
    "    relations = file_resp.get('related_files')\n",
    "    workflows = file_resp.get('workflow_run_inputs')\n",
    "    first_alias = file_resp.get('aliases',[None])[0]\n",
    "    # get related file\n",
    "    paired_file = ''\n",
    "    for relation in relations:\n",
    "        if relation['relationship_type'] == 'paired with':\n",
    "            paired_file = relation['file']['accession']\n",
    "    # Check workflows workflow partA\n",
    "    last_part_A = ''\n",
    "    last_part_A_status = 'did_not_run'\n",
    "    # Assumes workflow_runs come in time ordered list, and grabs the last ones for each wf run\n",
    "    if workflows:\n",
    "        for wfr_resp in workflows:\n",
    "            wfr_name = wfr_resp['display_title']   \n",
    "            if wfr_name.startswith(wf_partA):\n",
    "                last_part_A=wfr_resp['uuid']\n",
    "                last_part_A_status = wfr_resp.get('run_status')  \n",
    "    # return a small report\n",
    "    return {'file': file_id,\n",
    "            'alias': first_alias,\n",
    "            'paired_file': paired_file,\n",
    "            'last_part_A': last_part_A,\n",
    "            'last_part_A_status': last_part_A_status\n",
    "           }\n",
    "\n",
    "\n",
    "def make_hicb_json(input_files, env, output_bucket, accession, ncores):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"b9829418-49e5-4c33-afab-9ec90d65faf3\",\n",
    "                  \"app_name\": \"hi-c-processing-partb/23\",\n",
    "                  \"parameters\": {\n",
    "                      \"ncores\" :  ncores,\n",
    "                      \"binsize\": 5000,\n",
    "                      \"min_res\": 5000,\n",
    "                      \"normalization_type\": \"KR\"\n",
    "                  },\n",
    "                  \"_tibanna\": {\"env\": env, \"run_type\": \"hic-partb\", \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    \n",
    "    if not isinstance(obj_ids, list):\n",
    "        input_is_array = False\n",
    "        obj_ids = [ obj_ids ]\n",
    "        \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "     \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            print tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket)\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])\n",
    "            \n",
    "    if not input_is_array: \n",
    "        uuid_list = uuid_list[0]\n",
    "    if not input_is_array:\n",
    "        object_key_list = object_key_list[0]\n",
    "        \n",
    "    data = {'bucket_name' : bucket,\n",
    "                    'object_key' :  object_key_list,\n",
    "                    'uuid' : uuid_list,\n",
    "                    'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    input_is_array = True\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    \n",
    "    if not isinstance(obj_ids, list):\n",
    "        input_is_array = False\n",
    "        obj_ids = [ obj_ids ]\n",
    "        \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "     \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])\n",
    "            \n",
    "    if not input_is_array:\n",
    "        uuid_list = uuid_list[0]\n",
    "        object_key_list = object_key_list[0]\n",
    "        \n",
    "    data = {'bucket_name' : bucket,\n",
    "                    'object_key' :  object_key_list,\n",
    "                    'uuid' : uuid_list,\n",
    "                    'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbo_sets = [\n",
    "'dciclab:rao_rep02',\n",
    "'dciclab:rao_rep12'\n",
    "]\n",
    "\n",
    "all_sets = [\n",
    "            'dciclab:rao_rep07',\n",
    "            'dciclab:rao_rep02',\n",
    "            'dciclab:rao_rep12',\n",
    "            'dciclab:rao_rep13',\n",
    "            'dcic:Selvaraj_gm12878_hic',\n",
    "            'dcic:Jin_imr90_hic'\n",
    "            ]\n",
    "\n",
    "small_sets = [\n",
    "            'dcic:Selvaraj_gm12878_hic'\n",
    "            ]\n",
    "\n",
    "all_pairs = []\n",
    "for a_set in small_sets:\n",
    "    print 'report on', a_set\n",
    "    rep_resp = ff_utils.get_metadata(a_set, connection=ff)['experiments_in_set']\n",
    "    total = 0\n",
    "    set_pairs =[]\n",
    "    set_pairs_fine = True  \n",
    "    for exp in rep_resp:\n",
    "        # print 'Experiment', exp\n",
    "        exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "        exp_files = exp_resp['files']\n",
    "        for fastq_file in exp_files:\n",
    "            file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded') \n",
    "            # skip unfortunate status\n",
    "            if file_resp['status'] in ['deleted', 'uploading', 'upload failed']:\n",
    "                continue\n",
    "            # if no uploaded file in the file item report and skip\n",
    "            if not file_resp.get('filename'):\n",
    "                print file_resp['accession'], \"does not have a file\"\n",
    "                continue\n",
    "            # check if file is in s3\n",
    "            if not tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket):\n",
    "                print file_resp['accession'], \"does not have a file in S3\"\n",
    "                continue \n",
    "            # skip miseq\n",
    "            if file_resp.get('instrument') == \"Illumina MiSeq\":\n",
    "                continue\n",
    "            # skip pair no 2\n",
    "            if file_resp.get('paired_end')=='2':\n",
    "                continue \n",
    " \n",
    "            # get report\n",
    "            file_info = summarize_file(file_resp)\n",
    "            # get report on paired file\n",
    "            paired_file = file_info['paired_file']\n",
    "            pair_file_resp = ff_utils.get_metadata(paired_file, connection=ff, frame='embedded')\n",
    "            pair_file_info = summarize_file(pair_file_resp)\n",
    "            \n",
    "            partAfine = False\n",
    "            # check the partA for both paired fastq\n",
    "            if file_info['last_part_A_status'] == 'complete':\n",
    "                if pair_file_info['last_part_A_status'] == 'complete':\n",
    "                    if file_info['last_part_A'] == pair_file_info['last_part_A']:\n",
    "                        partAfine = True\n",
    "                        \n",
    "            ########\n",
    "            #######\n",
    "            #  We need to add a check if partV run before, so don't run twice\n",
    "            #####\n",
    "            ####\n",
    "            ###\n",
    "            ##\n",
    "            #\n",
    "            \n",
    "            hindIII_correction = False\n",
    "            if not partAfine:\n",
    "                set_pairs_fine = False\n",
    "                print file_info['file'], pair_file_info['file'], \"has problems with partA\"\n",
    "                print \"this experiment set will not be part of all_pairs list\"\n",
    "            else:\n",
    "                partA_data = ff_utils.get_metadata(file_info['last_part_A'], connection=ff)\n",
    "                inputs = partA_data.get('input_files')\n",
    "                # An old version of the hindIII restriction file needed to be corrected\n",
    "                # If found that as the input file, look at the wfr on pair file and use the \n",
    "                # correct one\n",
    "                nz = [i['value'] for i in inputs if i['workflow_argument_name']=='restriction_file'][0]\n",
    "                if nz == '/files-reference/4DNFI823L811/':\n",
    "                    hindIII_correction = True\n",
    "                outputs = partA_data.get('output_files')\n",
    "                pair_file = [i['value'] for i in outputs if i['format'] == 'pairs'][0]\n",
    "                pair_resp = ff_utils.get_metadata(pair_file, connection=ff, frame='embedded')  \n",
    "                \n",
    "                # check if file is in s3\n",
    "                head_info = tibanna.s3.does_key_exist(pair_resp['upload_key'], tibanna.s3.outfile_bucket)\n",
    "                if not head_info:\n",
    "                    set_pairs_fine = False\n",
    "                    print pair_resp['accession'], \"does not have a file in S3, skipping this set\"\n",
    "                    continue\n",
    "                \n",
    "                ### if there was a correction for partA, find the right pairs file\n",
    "                if hindIII_correction:\n",
    "                    pair_wfrs = pair_resp['workflow_run_inputs']\n",
    "                    if pair_wfrs:\n",
    "                        for wfr_resp in pair_wfrs:\n",
    "                            wfr_name = wfr_resp['display_title']   \n",
    "                            if wfr_name.startswith('hi-c-processing-parta-juicer-patch'):\n",
    "                                last_part_A_cor=wfr_resp['uuid']\n",
    "                                last_part_A_cor_status = wfr_resp.get('run_status') \n",
    "                    if last_part_A_cor_status != 'complete':\n",
    "                        set_pairs_fine = False\n",
    "                        print \"There is a problem with partA_correction run for pair\", pair_resp['accession']\n",
    "                        continue\n",
    "                    else:\n",
    "                        cor_wfr = ff_utils.get_metadata(last_part_A_cor, connection=ff)\n",
    "                        cor_outputs = cor_wfr.get('output_files')\n",
    "                        cor_pair_file = [i['value'] for i in cor_outputs if i['format'] == 'pairs'][0]\n",
    "                        cor_pair_resp = ff_utils.get_metadata(cor_pair_file, connection=ff)  \n",
    "                        # check if file is in s3\n",
    "                        cor_head_info = tibanna.s3.does_key_exist(cor_pair_resp['upload_key'], tibanna.s3.outfile_bucket)\n",
    "                        if not cor_head_info:\n",
    "                            set_pairs_fine = False\n",
    "                            print pair_resp['accession'], \"does not have the corrected pair file\", cor_pair_resp['accession'] ,\"in S3, skipping this set\"\n",
    "                            continue\n",
    "                        \n",
    "                        cor_file_size = round(cor_head_info['ContentLength']/1073741824.0,1)\n",
    "                        total += cor_file_size\n",
    "                        print cor_pair_file, str(cor_file_size), \"(corrected)\"\n",
    "                        set_pairs.append(cor_pair_file)\n",
    "                else:\n",
    "                    file_size = round(head_info['ContentLength']/1073741824.0,1)\n",
    "                    total += file_size\n",
    "                    print pair_file, str(file_size)\n",
    "                    set_pairs.append(pair_file)\n",
    "\n",
    "\n",
    "                \n",
    "    if set_pairs_fine:\n",
    "        all_pairs.append([a_set, set_pairs])\n",
    "    print str(total) + \"GB total file size\"\n",
    "    print\n",
    "\n",
    "print \"no of pairs sets\"\n",
    "print len(all_pairs)\n",
    "print all_pairs\n",
    "print \"use 'all_pairs' to pass the list of pairs files of each set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "from core import ff_utils\n",
    "import time\n",
    "\n",
    "# testportal\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "test_pairs = [\n",
    "    ['small_set_1.2gb_1.2gb', ['/files-processed/4DNFIO9EV5ME/', '/files-processed/4DNFIPZZNRT5/']],\n",
    "    ['medium_set_2.3gb_2.4gb',['/files-processed/4DNFI0U1SFJJ/', '/files-processed/4DNFI1C97MZ6/']],\n",
    "    ['large_set_3.6gb_3.6gb',['/files-processed/4DNFIXV3ACPK/', '/files-processed/4DNFI6NDND1Y/']]\n",
    "]\n",
    "\n",
    "\n",
    "for set_name, pair_list in all_pairs:\n",
    "    print set_name\n",
    "    print pair_list\n",
    "    chrsizes = make_input_file_json('4DNFI823LSII', 'chrsizes', tibanna, raw_file_bucket)\n",
    "    pair_files= make_input_file_json(pair_list, 'input_pairs', tibanna, output_file_bucket)\n",
    "    ncores = 8\n",
    "\n",
    "    # ncore options 8  cores up to 20gb per .fastq.gz (1tb)\n",
    "    #               36 cores up to 42gb per .fastq.gz (2tb)\n",
    "    #     Not set            up to 90gb per .fastq.gz (4tb)\n",
    "\n",
    "    input_files = [chrsizes, pair_files]\n",
    "    if all(input_files):\n",
    "        name = 'partB_'+ set_name.replace(\":\", \"_\")\n",
    "        input_json = make_hicb_json(input_files, env, output_file_bucket, name, ncores)\n",
    "        print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    #time.sleep(30)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
