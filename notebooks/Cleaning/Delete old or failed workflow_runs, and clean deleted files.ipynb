{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This script looks at all files\n",
    "### 1) Checks deleted files for md5 related fields and clears them\n",
    "###                         for qc metric and changes status of qc metric object to deleted\n",
    "###                         clears the qc metric field\n",
    "###                         for workflows and deletes all of them\n",
    "### 2) Checks other files for workflows and deleted old workflows\n",
    "###                                     and deleted problematic ones (status or rev)\n",
    "### takes around 20 min\n",
    "\n",
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "from datetime import datetime\n",
    "\n",
    "# set enviroment and key/connection\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "\n",
    "def get_wfr_report(wfrs):\n",
    "    wfr_report = []\n",
    "    for wfr_data in wfrs:\n",
    "        wfr_rep = {}\n",
    "        \"\"\"For a given workflow_run_sbg item, grabs details, uuid, run_status, wfr name, date, and run time\"\"\"\n",
    "        wfr_uuid = wfr_data['uuid']\n",
    "        wfr_data = ff_utils.get_metadata(wfr_uuid, connection = ff)\n",
    "        wfr_status = wfr_data['run_status']\n",
    "        try:\n",
    "            wfr_name = wfr_data['title'].split(' run ')[0]\n",
    "        except:\n",
    "            print('ProblematicCase')\n",
    "            print(wfr_data['uuid'], wfr_data.get('display_title', 'no title'))\n",
    "            continue\n",
    "        wfr_time = datetime.strptime(wfr_data['date_created'],'%Y-%m-%dT%H:%M:%S.%f+00:00')\n",
    "        run_hours = (datetime.utcnow()-wfr_time).total_seconds()/3600\n",
    "        wfr_name_list = wfr_data['title'].split(' run ')[0].split('/')\n",
    "        wfr_name = wfr_name_list[0]\n",
    "        try:\n",
    "            wfr_rev = wfr_name_list[1] \n",
    "        except:\n",
    "            wfr_rev = \"0\"\n",
    "\n",
    "        output_files = wfr_data.get('output_files',None)\n",
    "        output_uuids = []\n",
    "        if output_files:\n",
    "            for i in output_files:\n",
    "                if i.get('value', None):\n",
    "                    output_uuids.append(i['value'])\n",
    "\n",
    "        wfr_rep = {'wfr_uuid': wfr_data['uuid'],\n",
    "                   'wfr_status': wfr_data['run_status'],\n",
    "                   'wfr_name': wfr_name,\n",
    "                   'wfr_rev': wfr_rev,\n",
    "                   'wfr_date': wfr_time,\n",
    "                   'run_time': run_hours,\n",
    "                   'status': wfr_data['status'],\n",
    "                   'outputs': output_uuids}\n",
    "        wfr_report.append(wfr_rep)\n",
    "    wfr_report = sorted(wfr_report, key=lambda k: (k['wfr_date'], k['wfr_name']))\n",
    "    return wfr_report\n",
    "\n",
    "    \n",
    "def printTable(myDict, colList=None):\n",
    "    \"\"\" Pretty print a list of dictionaries Author: Thierry Husson\"\"\"\n",
    "    if not colList: colList = list(myDict[0].keys() if myDict else [])\n",
    "    myList = [colList] # 1st row = header\n",
    "    for item in myDict: myList.append([str(item[col] or '') for col in colList])\n",
    "    colSize = [max(map(len,col)) for col in zip(*myList)]\n",
    "    formatStr = ' | '.join([\"{{:<{}}}\".format(i) for i in colSize])\n",
    "    myList.insert(1, ['-' * i for i in colSize]) # Seperating line\n",
    "    for item in myList: print(formatStr.format(*item))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at 2018-02-26 20:52:15.626613\n",
      "Do you want to delete old workflowruns (if not, only report will be displayed (y/n))\n",
      "4717 files in the system\n",
      "100 0\n",
      "200 0\n",
      "300 0\n",
      "400 0\n",
      "500 0\n",
      "600 0\n",
      "700 0\n",
      "800 0\n",
      "900 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-77ce96906380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_with_deleted_wfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mraw_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embedded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mdeleted_wf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mwfr_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/Github/tibanna/core/ff_utils.pyc\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(obj_id, key, connection, frame)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;31m# default to always get from database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdn_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdnDCIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_FDN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0msleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/.virtualenvs/tibanna/lib/python2.7/site-packages/wranglertools/fdnDCIC.pyc\u001b[0m in \u001b[0;36mget_FDN\u001b[0;34m(obj_id, connection, frame, url_addon)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murl_addon\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@graph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@graph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/.virtualenvs/tibanna/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "print 'started at', datetime.utcnow()\n",
    "delete_workflows = raw_input(\"Do you want to delete old workflowruns (if not, only report will be displayed (y/n))\")\n",
    "\n",
    "# what kind of files should be searched for worflow run inputs, use url compatible naming\n",
    "\n",
    "# accepted workflows\n",
    "# workflow name, accepted revision numbers (0 if none), accetable run time (hours)\n",
    "workflow_details = [\n",
    "                    ['md5', ['0'], 24],\n",
    "                    ['fastqc-0-11-4-1', ['0', '1'], 24],\n",
    "                    ['hi-c-processing-parta-juicer', ['25','26'], 150],\n",
    "                    ['hi-c-processing-partb', ['31', '34', '38'],150],\n",
    "                    ['hi-c-processing-partc', ['3', '8'], 150],\n",
    "                    ['add-hic-normvector-to-mcool', ['3'], 150],\n",
    "                    ['extract-mcool-normvector-for-juicebox', ['100'], 150],\n",
    "                    ['extract-mcool-normvector-for-juicebox-1', ['1'], 150],\n",
    "                    ['bwa-mem', ['0'], 150],\n",
    "                    ['pairsam-parse-sort',['0'],150],\n",
    "                    ['pairsam-merge',['0'],150],\n",
    "                    ['pairsam-markasdup',['0'],150],\n",
    "                    ['pairsam-filter',['0'],150],\n",
    "                    ['addfragtopairs',['0'],150],\n",
    "                    ['pairs-patch',['0'],150],\n",
    "                    ['hi-c-processing-partb set',['0'],150],\n",
    "                    ['hi-c-processing-partb exp',['0'],150],\n",
    "                    ['hi-c-processing-partc set',['0'],150],\n",
    "                    ['hi-c-processing-partc exp',['0'],150],\n",
    "                    ['bwa-mem 0.2.5', ['0'],150],\n",
    "                    ['hi-c-processing-pairs 0.2.5', ['0'],150],\n",
    "                    ['pairsqc-single 0.2.5', ['0'],150],\n",
    "                    ['hi-c-processing-bam 0.2.5', ['0'],150],    \n",
    "                   ]\n",
    "workflow_names = [i[0] for i in workflow_details]\n",
    "\n",
    "deleted_wfr_no = 0\n",
    "files_with_deleted_wfr = 0\n",
    "\n",
    "files = [i['uuid'] for i in ff_utils.get_metadata('files-processed' , connection=ff)['@graph']]\n",
    "\n",
    "# #files = [u'311b0cbc-079e-4fd3-bd95-17df5e838a25', u'3f7c6c9b-741f-4a89-947b-d65b7ed77f81', \n",
    "#                 u'd7d771ba-a273-440e-9f73-41fb93005307', u'c05e8866-35de-4488-95de-af9fe6bfada6',\n",
    "#                 u'73e64de4-b773-46f3-a6fa-50e14815a076', u'e2cc7cae-6184-487a-b70c-6c6beb7f1ee5', \n",
    "#                 u'cd8385a5-4ab7-467e-8cd2-1fa9c24413d5', u'2db2d085-f1f0-4e78-a0ba-ce7faa60adbb', \n",
    "#                 u'3c22e002-d69b-4c78-9a50-4cc85417daa3', u'61dafc84-7991-4542-a3f8-645adee368af', \n",
    "#                 u'e3ba43cc-428f-4289-891d-96f698fbf7a5', u'0e449082-98d0-4c84-abb0-b49b661467dc', \n",
    "#                 u'5fb57fab-f2a7-4fac-a6bd-fd2ce0e28eb2', u'695212e9-64a7-4d55-a29f-f7895108bd4f']\n",
    "\n",
    "print len(files), 'files in the system'\n",
    "deleted_wfrs = []\n",
    "counter = 0\n",
    "del_md5 = 0\n",
    "del_qc = 0\n",
    "deleted_output = 0\n",
    "for a_file in files:\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print counter, files_with_deleted_wfr\n",
    "    raw_file = ff_utils.get_metadata(a_file, connection = ff, frame='embedded')\n",
    "    deleted_wf = False\n",
    "    wfr_report = []\n",
    "    wfrs = raw_file.get('workflow_run_inputs')\n",
    "    \n",
    "    # Delete wfrs if file is deleted\n",
    "    if raw_file['status'] == 'deleted':\n",
    "        if delete_workflows.lower() in ['y', 'yes']:\n",
    "            # clean deleted files of md5 and qc metrics\n",
    "            for a_field in ['content_md5sum', 'md5sum']:  \n",
    "                if raw_file.get(a_field):\n",
    "                    ff_utils.delete_field(raw_file, a_field, connection=ff)\n",
    "                    del_md5 += 1\n",
    "            if raw_file.get('quality_metric'):\n",
    "                qc_uuid = raw_file['quality_metric']['uuid']\n",
    "                ff_utils.delete_field(raw_file, 'quality_metric', connection=ff)\n",
    "                # delete quality metrics object\n",
    "                patch_data = {'status': \"deleted\"}\n",
    "                ff_utils.patch_metadata(patch_data, obj_id=qc_uuid ,connection=ff)\n",
    "                del_qc += 1\n",
    "        # delete all workflows for deleted files\n",
    "        if not wfrs:\n",
    "            continue\n",
    "        else:\n",
    "            wfr_report = get_wfr_report(wfrs)\n",
    "            for wfr_to_del in wfr_report:\n",
    "                if wfr_to_del['status'] != 'deleted':\n",
    "                    if wfr_to_del['wfr_name'] not in workflow_names:\n",
    "                        print('Unlisted Workflow', wfr_to_del['wfr_name'], 'deleted file workflow', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                    deleted_wf = True\n",
    "                    deleted_wfr_no += 1\n",
    "                    \n",
    "                    ####################################################\n",
    "                    ## TEMPORARY PIECE##################################\n",
    "                    if wfr_to_del['status'] == 'released to project':\n",
    "                        print('saved from deletion', wfr_to_del['wfr_name'], 'deleted file workflow', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                        continue\n",
    "                    if wfr_to_del['status'] == 'released':\n",
    "                        print('delete released!!!!!', wfr_to_del['wfr_name'], 'deleted file workflow', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                        continue  \n",
    "                    #####################################################\n",
    "                    \n",
    "                    print(wfr_to_del['wfr_name'], 'deleted file workflow', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                    if delete_workflows.lower() in ['y', 'yes']:\n",
    "                        patch_data = {'description': \"This workflow run is deleted\", 'status': \"deleted\"}\n",
    "                        deleted_wfrs.append(wfr_to_del['wfr_uuid'])\n",
    "                        ff_utils.patch_metadata(patch_data, obj_id=wfr_to_del['wfr_uuid'] ,connection=ff)\n",
    "                        # delete output files of the deleted workflow run\n",
    "                        if wfr_to_del['outputs']:\n",
    "                            for out_file in wfr_to_del['outputs']:\n",
    "                                deleted_output += 1\n",
    "                                ff_utils.patch_metadata({'status': \"deleted\"}, obj_id=out_file ,connection=ff)\n",
    "       \n",
    "                \n",
    "    else:\n",
    "        # get a report on all workflow_runs\n",
    "        if not wfrs:\n",
    "            continue\n",
    "        else:\n",
    "            wfr_report = get_wfr_report(wfrs)\n",
    "            # printTable(wfr_report, ['wfr_name', 'run_time', 'wfr_rev', 'run_time', 'wfr_status'])\n",
    "            \n",
    "            # check if any unlisted wfr in report\n",
    "            my_wfr_names = [i['wfr_name'] for i in wfr_report]\n",
    "            unlisted = [x for x in my_wfr_names if x not in workflow_names]\n",
    "            if unlisted:\n",
    "                print('Unlisted Workflow', unlisted, 'skipped in', raw_file['accession'])\n",
    "                    \n",
    "            for wf_name,accepted_rev,accepted_run_time in workflow_details:\n",
    "                #for each type of worklow make a list of old ones, and patch status and description\n",
    "                sub_wfrs = [i for i in wfr_report if i['wfr_name'] == wf_name]\n",
    "                if sub_wfrs:\n",
    "                    active_wfr = sub_wfrs[-1]\n",
    "                    old_wfrs = sub_wfrs [:-1]\n",
    "                    # check the status of the most recent workflow\n",
    "                    if active_wfr['wfr_status'] != 'complete':\n",
    "                        if active_wfr['wfr_status'] in ['running', 'started'] and active_wfr['run_time'] < accepted_run_time:\n",
    "                            print wf_name,'still running for', a_file\n",
    "                        else:\n",
    "                            old_wfrs.append(active_wfr)\n",
    "                    elif active_wfr['wfr_rev'] not in accepted_rev:\n",
    "                        old_wfrs.append(active_wfr)\n",
    "                    if old_wfrs:\n",
    "                        for wfr_to_del in old_wfrs:\n",
    "                            if wfr_to_del['status'] != 'deleted':\n",
    "                                deleted_wf = True\n",
    "                                deleted_wfr_no += 1 \n",
    "                                \n",
    "                                ####################################################\n",
    "                                ## TEMPORARY PIECE\n",
    "                                if wfr_to_del['status'] == 'released to project':\n",
    "                                    print('saved from deletion',wfr_to_del['wfr_name'], 'old style or dub', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                                    continue\n",
    "                                if wfr_to_del['status'] == 'released':\n",
    "                                    print('delete released????',wfr_to_del['wfr_name'], 'old style or dub', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                                    continue\n",
    "                                ####################################################\n",
    "\n",
    "                                print(wfr_to_del['wfr_name'], 'old style or dub', wfr_to_del['wfr_uuid'], raw_file['accession'])\n",
    "                                \n",
    "                                if delete_workflows.lower() in ['y', 'yes']:\n",
    "                                    patch_data = {'description': \"This workflow run is deleted\", 'status': \"deleted\"}\n",
    "                                    deleted_wfrs.append(wfr_to_del['wfr_uuid'])\n",
    "                                    \n",
    "                                    ff_utils.patch_metadata(patch_data, obj_id=wfr_to_del['wfr_uuid'] ,connection=ff)\n",
    "                                    # delete output files of the deleted workflow run\n",
    "                                    if wfr_to_del['outputs']:\n",
    "                                        for out_file in wfr_to_del['outputs']:\n",
    "                                            deleted_output += 1\n",
    "                                            ff_utils.patch_metadata({'status': \"deleted\"}, obj_id=out_file ,connection=ff)\n",
    "    if deleted_wf:\n",
    "        files_with_deleted_wfr += 1\n",
    "\n",
    "if delete_workflows.lower() in ['y', 'yes']:\n",
    "    print str(deleted_wfr_no),\"workflowruns from\", str(files_with_deleted_wfr), \"files deleted\"\n",
    "else:\n",
    "    print str(deleted_wfr_no),\"workflowruns from\", str(files_with_deleted_wfr), \"files need to be deleted\"\n",
    "\n",
    "print len(deleted_wfrs)\n",
    "print del_md5, 'md5 fields deleted'\n",
    "print del_qc, 'qc metrics deleted'\n",
    "print deleted_output, 'deleted output files'\n",
    "print 'finished at', datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
