{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "from core.utils import run_workflow\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "raw_bucket = tibanna.s3.raw_file_bucket\n",
    "out_bucket = tibanna.s3.outfile_bucket\n",
    "exclude_miseq = True\n",
    "\n",
    "def extract_file_info(obj_id, arg_name):\n",
    "    \"\"\"Creates the formatted dictionary for input files.\n",
    "    \"\"\"\n",
    "    # start a dictionary\n",
    "    template = {\"workflow_argument_name\": arg_name}\n",
    "    \n",
    "    # if it is list of items, change the structure\n",
    "    if isinstance(obj_id, list):\n",
    "        object_key = []\n",
    "        uuid = []\n",
    "        buckets = []\n",
    "        for obj in obj_id:\n",
    "            metadata = ff_utils.get_metadata(obj, connection=ff)\n",
    "            object_key.append(metadata['display_title'])\n",
    "            uuid.append(metadata['uuid'])\n",
    "            # get the bucket\n",
    "            if 'FileProcessed' in metadata['@type']:\n",
    "                my_bucket = out_bucket\n",
    "            else:  # covers cases of FileFastq, FileReference, FileMicroscopy\n",
    "                my_bucket = raw_bucket\n",
    "            buckets.append(my_bucket)\n",
    "        # check bucket consistency\n",
    "        try:\n",
    "            assert len(list(set(buckets))) == 1\n",
    "        except:\n",
    "            print('Files from different buckets', obj_id)\n",
    "            return\n",
    "        template['object_key'] = object_key\n",
    "        template['uuid'] = uuid\n",
    "        template['bucket_name'] = buckets[0]\n",
    "    # if obj_id is a string\n",
    "    else:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "        template['object_key'] = metadata['display_title']\n",
    "        template['uuid'] = metadata['uuid']\n",
    "        # get the bucket\n",
    "        if 'FileProcessed' in metadata['@type']:\n",
    "            my_bucket = out_bucket\n",
    "        else:  # covers cases of FileFastq, FileReference, FileMicroscopy\n",
    "            my_bucket = raw_bucket\n",
    "        template['bucket_name'] = my_bucket\n",
    "    return template\n",
    "    \n",
    "\n",
    "def run_json(input_files, env, parameters, wf_uuid, wf_name, run_name):\n",
    "    \"\"\"Creates the trigger json that is used by tibanna.\n",
    "    \"\"\"\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': out_bucket,\n",
    "                  'workflow_uuid': wf_uuid,\n",
    "                  \"app_name\": wf_name,\n",
    "                  \"parameters\": parameters,\n",
    "                  \"config\": {\n",
    "                        \"ebs_type\": \"io1\",\n",
    "                        \"json_bucket\": \"4dn-aws-pipeline-run-json\",\n",
    "                        \"ebs_iops\": 500,\n",
    "                        \"shutdown_min\": 30,\n",
    "                        \"s3_access_arn\": \"arn:aws:iam::643366669028:instance-profile/S3_access\",\n",
    "                        \"ami_id\": \"ami-cfb14bb5\",\n",
    "                        \"copy_to_s3\": True,\n",
    "                        \"launch_instance\": True,\n",
    "                        \"password\": \"dragonfly\",\n",
    "                        \"log_bucket\": \"tibanna-output\",\n",
    "                        \"script_url\": \"https://raw.githubusercontent.com/4dn-dcic/tibanna/master/awsf/\",\n",
    "                        \"key_name\": \"4dn-encode\"\n",
    "                    },\n",
    "                  \"_tibanna\": {\"env\": env, \n",
    "                               \"run_type\": wf_name,\n",
    "                               \"run_id\": run_name},\n",
    "                  \"tag\": '0.2.5'\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "\n",
    "def find_pairs(my_rep_set):\n",
    "    \"\"\"Find pairs and make sure they are fine my qc.\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    rep_resp = my_rep_set['experiments_in_set']\n",
    "    enzymes = []\n",
    "    organisms = []\n",
    "    for exp in rep_resp:\n",
    "        exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "        report[exp_resp['accession']] = []\n",
    "        if not organisms:\n",
    "            biosample = ff_utils.get_metadata(exp_resp['biosample'], connection=ff, frame='embedded')      \n",
    "            organisms = list(set([bs['individual']['organism']['display_title'] for bs in biosample['biosource']]))\n",
    "            if len(organisms) != 1:\n",
    "                print 'multiple organisms in set', my_rep_set['accession']\n",
    "                break\n",
    "        exp_files = exp_resp['files']\n",
    "        enzyme = exp_resp.get('digestion_enzyme')\n",
    "        enzymes.append(enzyme)\n",
    "        for fastq_file in exp_files:\n",
    "            file_resp = ff_utils.get_metadata(fastq_file, connection=ff)  \n",
    "            # skip pair no 2\n",
    "            if file_resp.get('paired_end')=='2':\n",
    "                continue \n",
    "            # exclude miseq\n",
    "            if exclude_miseq:\n",
    "                if file_resp.get('instrument') == 'Illumina MiSeq':\n",
    "                    print 'skipping miseq files', exp\n",
    "                    continue\n",
    "                \n",
    "            #Some checks before running\n",
    "            #check if status is deleted\n",
    "            if file_resp['status'] == 'deleted':\n",
    "                print 'deleted file', file_resp['accession'], 'in', my_rep_set['accession']\n",
    "                continue\n",
    "            #if no uploaded file in the file item report and skip\n",
    "            if not file_resp.get('filename'):\n",
    "                print file_resp['accession'], \"does not have a file\"\n",
    "                continue\n",
    "            # check if file is in s3\n",
    "            head_info = tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket)\n",
    "            if not head_info:\n",
    "                print file_resp['accession'], \"does not have a file in S3\"\n",
    "                continue\n",
    "            # check that file has a pair\n",
    "            f1 = file_resp['@id']\n",
    "            f2 = ''\n",
    "            relations = file_resp.get('related_files')\n",
    "            for relation in relations:\n",
    "                if relation['relationship_type'] == 'paired with':\n",
    "                    f2 = relation['file']\n",
    "            if not f2:\n",
    "                print f1, 'does not have a pair'\n",
    "                continue\n",
    "            report[exp_resp['accession']].append((f1, f2))\n",
    "            \n",
    "    # get the organism\n",
    "    if len(list(set(organisms))) == 1:\n",
    "        organism = organisms[0]\n",
    "    else:\n",
    "        organism = None\n",
    "        print 'problematic organism', set(organisms)\n",
    "        \n",
    "    # get the enzyme\n",
    "    if len(list(set(enzymes))) == 1:\n",
    "        enz = enzymes[0].split('/')[2]\n",
    "    else:\n",
    "        enz = None\n",
    "        print 'problematic enzyme', set(enzymes)\n",
    "    return report, organism, enz\n",
    "\n",
    "\n",
    "def get_wfr_out(file_id, wfr_name, file_format):\n",
    "    emb_file = ff_utils.get_metadata(file_id, connection=ff, frame = 'embedded')\n",
    "    workflows = emb_file.get('workflow_run_inputs')\n",
    "    wfr = {}\n",
    "    run_status = 'did not run'\n",
    "    if workflows:\n",
    "        for a_wfr in workflows:\n",
    "            wfr_resp = ff_utils.get_metadata(a_wfr['uuid'], connection=ff)  \n",
    "            wfr_resp_name = wfr_resp['display_title']\n",
    "            if wfr_resp_name.startswith(wfr_name):\n",
    "                wfr = wfr_resp\n",
    "                run_status = wfr_resp['run_status']\n",
    "    else:\n",
    "        return \"no workflow in file\"\n",
    "    \n",
    "    if run_status == 'complete':\n",
    "        outputs = wfr.get('output_files')\n",
    "        file_id = [i['value'] for i in outputs if i['format'] == file_format][0]\n",
    "        if file_id:\n",
    "            return file_id\n",
    "        else:\n",
    "            return \"no file found\"\n",
    "    else:\n",
    "        return \"no completed run\"\n",
    "\n",
    "def add_processed_files(item_id, list_pc, ff):\n",
    "    # patch the exp or set\n",
    "    patch_data = {'processed_files': list_pc}\n",
    "    ff_utils.patch_metadata(patch_data, obj_id=item_id ,connection=ff)\n",
    "    return\n",
    "\n",
    "def release_files(set_id, list_items, ff):\n",
    "    item_status = ff_utils.get_metadata(set_id, connection=ff)['status']\n",
    "    # bring files to same status as experiments and sets\n",
    "    if item_status in ['released', 'released to project']:\n",
    "        for a_file in list_items:\n",
    "            it_resp = ff_utils.get_metadata(a_file, connection=ff)\n",
    "            workflow = it_resp.get('workflow_run_outputs')\n",
    "            # release the wfr that produced the file\n",
    "            if workflow:\n",
    "                ff_utils.patch_metadata({\"status\":item_status}, obj_id=workflow[0] ,connection=ff)  \n",
    "            ff_utils.patch_metadata({\"status\":item_status}, obj_id=a_file ,connection=ff)\n",
    "\n",
    "            \n",
    "def run_missing_wfr(wf_info, input_files, run_name ,ff):\n",
    "    all_inputs = []\n",
    "    for arg, files in input_files.iteritems():\n",
    "        inp = extract_file_info(files, arg)\n",
    "        all_inputs.append(inp)\n",
    "    wf_name = wf_info['wf_name']\n",
    "    wf_uuid = wf_info['wf_uuid']\n",
    "    parameters = wf_info['parameters']\n",
    "    input_json = run_json(all_inputs, env, parameters, wf_uuid, wf_name, run_name)\n",
    "    run_workflow(input_json)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "1 4DNES2R6PUEK\n",
      "DpnII human\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# for a given experiment set and some parameters like instrument\n",
    "# print set of files and their partA hic workflow status\n",
    "# if there are one that are running report the number of running cases\n",
    "# if there are file pairs that don't have a corresponding part A, report them separately\n",
    "\n",
    "wf_dict =[\n",
    "    {'wf_name': 'bwa-mem',\n",
    "     'wf_uuid': '3feedadc-50f9-4bb4-919b-09a8b731d0cc',\n",
    "     'parameters':{\"nThreads\": 16},\n",
    "    },\n",
    "    {'wf_name': 'hi-c-processing-bam',\n",
    "     'wf_uuid': '023bfb3e-9a8b-42b9-a9d4-216079526f68',\n",
    "     'parameters':{\"nthreads_merge\": 16, \"nthreads_parse_sort\": 16},\n",
    "    },\n",
    "    {'wf_name': 'hi-c-processing-pairs',\n",
    "     'wf_uuid': 'c9e0e6f7-b0ed-4a42-9466-cadc2dd84df0',\n",
    "     'parameters': {\"nthreads\": 1, \"maxmem\": \"32g\"},\n",
    "    }    \n",
    "]\n",
    "\n",
    "# url for hic exps\n",
    "exp_types = ['in%20situ%20Hi-C', 'dilution%20Hi-C']\n",
    "set_url = '/search/?'+'&'.join(['experiments_in_set.experiment_type='+i for i in exp_types])+'&type=ExperimentSetReplicate'\n",
    "# run_sets = ff_utils.get_metadata(set_url , connection=ff)['@graph']\n",
    "\n",
    "add_pc = False\n",
    "add_rel = False\n",
    "add_wfr = True\n",
    "\n",
    "test_set = '4DNES2R6PUEK'\n",
    "run_sets = [ff_utils.get_metadata(test_set , connection=ff)]\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "print len(run_sets)\n",
    "for a_set in run_sets: \n",
    "    if a_set.get('completed_processes') == [\"HiC_Pipeline_0.2.5\"]:\n",
    "        continue   \n",
    "    part3 = 'done'\n",
    "    list_release = []\n",
    "    set_pairs = []\n",
    "    counter += 1\n",
    "    print\n",
    "    print counter, a_set['accession']\n",
    "    fastqpairs, organism, enzyme = find_pairs(a_set)\n",
    "    print enzyme, organism\n",
    "    \n",
    "    if organism != 'human':\n",
    "        continue\n",
    "        \n",
    "    # cycle through the experiments\n",
    "    for exp in fastqpairs.keys():\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        part2 = 'done'\n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            bam1 = get_wfr_out(pair[0], 'bwa-mem 0.2.5', 'bam')\n",
    "            bam2 = get_wfr_out(pair[1], 'bwa-mem 0.2.5', 'bam')\n",
    "            # if run is not successful\n",
    "            if bam1.startswith('no') or not bam1 or bam1 != bam2:\n",
    "                part1 = 'not ready'\n",
    "                if add_wfr:\n",
    "                    # find the correct index\n",
    "                    if organism == 'human':\n",
    "                        bwa = '4DNFIZQZ39L9'\n",
    "#                     elif organism == 'mouse':\n",
    "#                         bwa = '4DNFI823LSI8'\n",
    "                    else:\n",
    "                        print 'not yet usable', organism\n",
    "                        continue\n",
    "\n",
    "                    inp_f = {'fastq1':pair[0], 'fastq2':pair[1], 'bwa_index':bwa}\n",
    "                    print inp_f\n",
    "                    #run_missing_wfr(wf_dict[0], inp_f, pickaname, ff) \n",
    "            # if successful\n",
    "            else:\n",
    "                exp_bams.append(bam1)\n",
    "                list_release.append(bam1)\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print exp, 'has missing Part1 runs'\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print exp, 'part1 complete'\n",
    "        #check if part 2 is run already, it not start the run\n",
    "        exp_com_bam = []\n",
    "        exp_pairs = []\n",
    "        for bam in exp_bams:\n",
    "            com_bam = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'bam')\n",
    "            pairs = get_wfr_out(bam, 'hi-c-processing-bam 0.2.5', 'pairs')\n",
    "            # try to run if missing\n",
    "            if pairs.startswith('no') or not pairs:\n",
    "                part2 = 'not ready'   \n",
    "            else:\n",
    "                exp_com_bam.append(com_bam)\n",
    "                exp_pairs.append(pairs)\n",
    "        \n",
    "        # make sure all bams went through the same wfr and produces same file\n",
    "        if part2 != 'done' or len(list(set(exp_com_bam))) != 1 or len(list(set(exp_pairs))) !=1:\n",
    "            print exp, 'Part2 did not complete'\n",
    "            part3 = 'not ready' \n",
    "        \n",
    "            if add_wfr:\n",
    "                # find the correct chrsize\n",
    "                if organism == 'human':\n",
    "                    chrsize = '4DNFI823LSII'\n",
    "#                     elif organism == 'mouse':\n",
    "#                         chrsize = ''\n",
    "                else:\n",
    "                    print 'not yet usable', organism\n",
    "                    continue\n",
    "                # make sure no duplicates\n",
    "                inp_f = {'input_bams':exp_bams, 'chromsize':chrsize}           \n",
    "                print inp_f\n",
    "                run_missing_wfr(wf_dict[1], inp_f, exp, ff)   \n",
    "            continue\n",
    "            \n",
    "        # add bam and pairs to exp proc file\n",
    "        list_release.extend([exp_com_bam[0],exp_pairs[0]])\n",
    "        if add_pc:\n",
    "            add_processed_files(exp, [exp_com_bam[0],exp_pairs[0]], ff)\n",
    "        \n",
    "        print exp, 'part2 complete'\n",
    "        set_pairs.append(exp_pairs[0])\n",
    "           \n",
    "    if part3 != 'done':\n",
    "        print 'Part3 not ready'\n",
    "        continue\n",
    "    merged_pairs = []\n",
    "    for set_pair in set_pairs:\n",
    "        merged_pair = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'pairs')\n",
    "        hic = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'hic')\n",
    "        mcool = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'mcool')\n",
    "        normvec = get_wfr_out(set_pair, 'hi-c-processing-pairs 0.2.5', 'normvector_juicerformat')\n",
    "        \n",
    "        if merged_pair.startswith('no') or not merged_pair:\n",
    "            part3 = 'part3 did not complete'              \n",
    "        else:\n",
    "            merged_pairs.append(merged_pair)\n",
    "                \n",
    "    if part3 != 'done' or len(list(set(merged_pairs))) != 1:\n",
    "        print a_set['accession'], 'is missing Part3'\n",
    "        part3 = 'not ready'\n",
    "        \n",
    "        if add_wfr:\n",
    "            # find the correct chrsize\n",
    "            if organism == 'human':\n",
    "                chrsize = '4DNFI823LSII'\n",
    "#                     elif organism == 'mouse':\n",
    "#                         chrsize = ''\n",
    "            else:\n",
    "                print 'not yet usable', organism\n",
    "                continue\n",
    "            # find enzyme\n",
    "            res_file = ''\n",
    "            if organism == 'human':\n",
    "                res_enzymes = {'MboI':'4DNFI823L812', 'DpnII':'4DNFIBNAPW30' ,'HindIII':'4DNFI823MBKE'}\n",
    "                res_file = res_enzymes.get(enzyme)\n",
    "                \n",
    "            if not res_file:\n",
    "                print 'restriction enzyme not ready', enzyme\n",
    "                continue\n",
    "            inp_f = {'input_pairs':set_pairs, 'chromsizes':chrsize, 'restriction_file': res_file}           \n",
    "            print inp_f\n",
    "            run_missing_wfr(wf_dict[1], inp_f, exp, ff)\n",
    "        \n",
    "        continue\n",
    "\n",
    "    #####\n",
    "    #add competed flag to experiment\n",
    "    if add_pc and add_rel:\n",
    "        ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.5\"]}, obj_id=a_set['accession'] ,connection=ff)\n",
    "    \n",
    "    # add processed files to set\n",
    "    list_release.extend([merged_pair, hic, mcool, normvec])\n",
    "    if add_pc:\n",
    "        add_processed_files(a_set['accession'], [merged_pair, hic, mcool, normvec], ff)\n",
    "    \n",
    "    #release files and wfrs\n",
    "    if add_rel:\n",
    "        release_files(a_set['accession'], list(set(list_release)), ff)\n",
    "    \n",
    "    completed += 1\n",
    "    completed_acc.append(a_set['accession'])\n",
    "    print a_set['accession'], 'part3 complete'\n",
    "    \n",
    "print completed\n",
    "print completed_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "[u'4DNESRJ8KV4Q', u'4DNES78Y8Y5K', u'4DNESB6MNCFE', u'4DNES8J78WV2', u'4DNESAPF27TG', u'4DNES9L4AK6Q', u'4DNES2M5JIGV', u'4DNESLLTENG9', u'4DNES98CI6GV', u'4DNESC5J3EIX', u'4DNES21NPLZU', u'4DNESYTIFTEE', u'4DNESIG4ELE4', u'4DNESNHN919R', u'4DNES8ZUV5CQ', u'4DNESCCP4KTY', u'4DNEST9AVULS', u'4DNES7DFQZLI', u'4DNESFBT9P4O', u'4DNESE3ICNE1', u'4DNES4GSP9S4', u'4DNESTAPSPUC', u'4DNESI2UKI7P', u'4DNESJ1VX52C', u'4DNESHGL976U', u'4DNESYUYFD6H', u'4DNESUTEOMGQ', u'4DNESOSE2FYZ', u'4DNESEW5JLUC', u'4DNESHFBC56P', u'4DNESDEK4IH8', u'4DNESI7DEJTM', u'4DNESUB35TII', u'4DNESIE5R9HS', u'4DNESSM1H92K', u'4DNES1ZEJNRU', u'4DNES4269GKX', u'4DNESPXW8XHY', u'4DNESLLA3R1V']\n"
     ]
    }
   ],
   "source": [
    "print completed\n",
    "print completed_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for upload key 1f53df95-4cf3-41cc-971d-81bb16c486dd/4DNFIZQZ39L9.bwaIndex.tgz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 4a6d10ee-2edb-4402-a98f-0edb1d58f5e1/4DNFI823LSI8.bwaIndex.tgz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 0cfb16e8-b902-4977-a498-587d36497687/4DNFIG22ZQ7Y.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 37d96da6-1daf-4bd0-87fa-50ec8df1cf3f/4DNFIMM81AZ3.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFIG22ZQ7Y-4DNFIMM81AZ3\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 10, 288000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '6da9735e-2bd5-11e8-91f9-51cdf96108a6', 'HTTPHeaders': {'x-amzn-requestid': '6da9735e-2bd5-11e8-91f9-51cdf96108a6', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIG22ZQ7Y-4DNFIMM81AZ3'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIG22ZQ7Y-4DNFIMM81AZ3\n",
      "looking for upload key 682971b6-eef4-49ca-b35d-7c7b6475bc28/4DNFI855FM8W.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key e230900b-98f5-4473-aecb-9f866c44576b/4DNFIHIB8POP.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFI855FM8W-4DNFIHIB8POP\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 16, 767000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '7186ad32-2bd5-11e8-b840-ff6a6d6d0caa', 'HTTPHeaders': {'x-amzn-requestid': '7186ad32-2bd5-11e8-b840-ff6a6d6d0caa', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI855FM8W-4DNFIHIB8POP'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI855FM8W-4DNFIHIB8POP\n",
      "looking for upload key 97f08476-c5eb-4257-85d4-03a7cec555b8/4DNFISUM635J.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 833e5ba8-ea5b-4cf5-a2fb-baa3f46b70c0/4DNFIWE1ZP5N.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFISUM635J-4DNFIWE1ZP5N\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 23, 743000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '75adc1a9-2bd5-11e8-833a-33c97d378779', 'HTTPHeaders': {'x-amzn-requestid': '75adc1a9-2bd5-11e8-833a-33c97d378779', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFISUM635J-4DNFIWE1ZP5N'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFISUM635J-4DNFIWE1ZP5N\n",
      "looking for upload key a508702f-c0b2-4774-b865-07a3dfdb3657/4DNFITYRLYTG.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 6de300ba-840d-4b45-bf91-98bab76a3294/4DNFI5J8F7AT.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFITYRLYTG-4DNFI5J8F7AT\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 30, 920000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '79f64111-2bd5-11e8-ae4b-3b54587c1e8d', 'HTTPHeaders': {'x-amzn-requestid': '79f64111-2bd5-11e8-ae4b-3b54587c1e8d', 'content-length': '141', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFITYRLYTG-4DNFI5J8F7AT'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFITYRLYTG-4DNFI5J8F7AT\n",
      "looking for upload key 5fb1afa0-6256-4f6a-afde-25405c931631/4DNFIXP926SF.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 0daac204-aaeb-4e37-bf84-4b3d6f17d59a/4DNFI2VXKAOI.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFIXP926SF-4DNFI2VXKAOI\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 38, 922000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '7eba590f-2bd5-11e8-9b16-9d9901527bc7', 'HTTPHeaders': {'x-amzn-requestid': '7eba590f-2bd5-11e8-9b16-9d9901527bc7', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIXP926SF-4DNFI2VXKAOI'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFIXP926SF-4DNFI2VXKAOI\n",
      "looking for upload key 7b5c4d95-4b6a-48af-9028-b8947b9ba8f2/4DNFI6W569F6.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "looking for upload key 88e59e41-888e-4cef-8944-e3f2ffb45826/4DNFI923YLXB.fastq.gz, on bucket elasticbeanstalk-fourfront-webprod-files\n",
      "about to start run bwa-mem_4DNFI6W569F6-4DNFI923YLXB\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2018, 3, 19, 20, 27, 46, 774000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '8369afc3-2bd5-11e8-9f46-599974d1ccfd', 'HTTPHeaders': {'x-amzn-requestid': '8369afc3-2bd5-11e8-9f46-599974d1ccfd', 'content-length': '142', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI6W569F6-4DNFI923YLXB'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:bwa-mem_4DNFI6W569F6-4DNFI923YLXB\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "import time\n",
    "\n",
    "paired_files = pairs_ready_to_run\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "outfiles = tibanna.s3.outfile_bucket\n",
    "tibanna.s3.outfile_bucket = 'elasticbeanstalk-fourfront-webprod-files'\n",
    "index_h = make_input_file_json('4DNFIZQZ39L9', 'bwa_index', tibanna)\n",
    "index_m = make_input_file_json('4DNFI823LSI8', 'bwa_index', tibanna)\n",
    "\n",
    "for set_name, organisms, f1,f2 in paired_files: \n",
    "\n",
    "    # find the correct index\n",
    "    if organisms == ['human']:\n",
    "        index = index_h\n",
    "    elif organisms == ['mouse']:\n",
    "        #index = index_m\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    fastq1 = make_input_file_json(f1, 'fastq1', tibanna)\n",
    "    fastq2 = make_input_file_json(f2, 'fastq2', tibanna)\n",
    "\n",
    "    input_files = [fastq1, fastq2, index]\n",
    "    if all(input_files):\n",
    "        name = fastq1['object_key'].split('.')[0] + \"-\" + fastq2['object_key'].split('.')[0]\n",
    "        input_json = make_hic1_json(input_files, env, outfiles, name)\n",
    "        # print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    time.sleep(2)\n",
    "    #a = raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
