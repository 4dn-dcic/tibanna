{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "\n",
    "#format for input json in hic-partA\n",
    "def make_input_file_json(obj_id, arg_name, tibanna):\n",
    "    '''\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "    data = {}\n",
    "    \n",
    "    # just make sure the file is on s3, otherwise bail\n",
    "    print(\"looking for upload key %s, on bucket %s\" % \n",
    "          (metadata['upload_key'],\n",
    "           tibanna.s3.outfile_bucket))\n",
    "    if tibanna.s3.does_key_exist(metadata['upload_key']):\n",
    "        data = {'bucket_name' : tibanna.s3.outfile_bucket,\n",
    "                'object_key' : metadata['upload_key'].split('/')[1],\n",
    "                'uuid' : metadata['uuid'],\n",
    "                'workflow_argument_name': arg_name\n",
    "                }\n",
    "    return data\n",
    "    \n",
    "\n",
    "def make_hica_json(input_files, env, output_bucket, accession, ncores):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"a9caf6f3-49e5-4c33-bfab-9ec90d65111c\",\n",
    "                  \"app_name\": \"hi-c-processing-parta-juicer/16\",\n",
    "                  \"parameters\": {\n",
    "                      \"nsplit\": 800,\n",
    "                      \"ncores\" : ncores\n",
    "                      },\n",
    "                  \"_tibanna\": {\"env\": env, \"run_type\": \"hic-parta\",\n",
    "                               \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you wanna run md5 and/or fastqc if missing? (md5/qc/all/none)\n",
      "a\n",
      "a\n",
      "b\n",
      "b\n",
      "c\n",
      "/files-fastq/4DNFI2ODUV3V/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "<html><head>\n",
      "<title>500 Internal Server Error</title>\n",
      "</head><body>\n",
      "<h1>Internal Server Error</h1>\n",
      "<p>The server encountered an internal error or\n",
      "misconfiguration and was unable to complete\n",
      "your request.</p>\n",
      "<p>Please contact the server administrator at \n",
      " root@localhost to inform them of the time this error occurred,\n",
      " and the actions you performed just before this error.</p>\n",
      "<p>More information about this error may be available\n",
      "in the server error log.</p>\n",
      "</body></html>\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8aa6adcf5e7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mfastq_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mfile_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastq_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embedded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m#Some checks before running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/Github/tibanna/core/ff_utils.pyc\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(obj_id, key, connection, frame)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdn_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdnDCIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_FDN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0msleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/.virtualenvs/tibanna/lib/python2.7/site-packages/wranglertools/fdnDCIC.pyc\u001b[0m in \u001b[0;36mget_FDN\u001b[0;34m(obj_id, connection, frame, url_addon)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murl_addon\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@graph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@graph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koray/.virtualenvs/tibanna/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "from tasks import run_md5\n",
    "from tasks import run_fastqc\n",
    "from invoke import run\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# for a given experiment set and some parameters like instrument\n",
    "# print set of files and their partA hic workflow status\n",
    "# if there are one that are running report the number of running cases\n",
    "# if there are file pairs that don't have a corresponding part A, report them separately\n",
    "\n",
    "all_sets = [\n",
    "            'dciclab:rao_rep07',\n",
    "            'dciclab:rao_rep02',\n",
    "            'dciclab:rao_rep12',\n",
    "            'dciclab:rao_rep13',\n",
    "            'dcic:Selvaraj_gm12878_hic',\n",
    "            'dcic:Jin_imr90_hic'\n",
    "            ]\n",
    "   \n",
    "my_rep_set = all_sets[0]\n",
    "exclude_miseq = True\n",
    "wf_md5 = \"md5\"\n",
    "wf_fastqc = \"fastqc-0-11-4-1/1\"\n",
    "wf_partA = \"hi-c-processing-parta-juicer/\"\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "run_md_qc = raw_input(\"Do you wanna run md5 and/or fastqc if missing? (md5/qc/all/none)\")\n",
    "\n",
    "# status for completion\n",
    "# there are two flavors of complete signals, before it was output_file_transfer_finished, not it is complete.\n",
    "# old completed wf runs have former one.\n",
    "status_done = ['complete', 'output_file_transfer_finished']\n",
    "\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "print 'a'\n",
    "rep_resp = ff_utils.get_metadata(my_rep_set, connection=ff)['experiments_in_set']\n",
    "print 'a'\n",
    "################\n",
    "##ADD TO WORKFLOW\n",
    "# wfr_time = datetime.strptime(wfr_data['date_created'],'%Y-%m-%dT%H:%M:%S.%f+00:00')\n",
    "# run_hours = int((datetime.now()-wfr_time).total_seconds()/3600)\n",
    "################\n",
    "\n",
    "def summarize_file(file_resp):\n",
    "    qc = False\n",
    "    file_id = file_resp['accession']\n",
    "    sequencer = file_resp.get('instrument')\n",
    "    relations = file_resp.get('related_files')\n",
    "    status = file_resp.get('status')\n",
    "    workflows = file_resp.get('workflow_run_inputs')\n",
    "    first_alias = file_resp.get('aliases',[None])[0]\n",
    "    pair_no = file_resp.get('paired_end')\n",
    "    # get related file\n",
    "    paired_file = ''\n",
    "    for relation in relations:\n",
    "        if relation['relationship_type'] == 'paired with':\n",
    "            paired_file = relation['file']['accession']\n",
    "    \n",
    "    # is there a qc?\n",
    "    if file_resp.get('quality_metric'):\n",
    "        qc = True\n",
    "    # Check workflows for qc fastqc workflow partA\n",
    "    last_part_A = ''\n",
    "    last_part_A_status = 'did_not_run'\n",
    "    md5_status = 'did_not_run'\n",
    "    fastqc_status = 'did_not_run'\n",
    "    # Assumes workflow_runs come in time ordered list, and grabs the last ones for each wf run\n",
    "    if workflows:\n",
    "        for wfr_resp in workflows:\n",
    "            wfr_name = wfr_resp['display_title']\n",
    "            if wfr_name.startswith(wf_md5):\n",
    "                md5_status = wfr_resp.get('run_status')     \n",
    "            elif wfr_name.startswith(wf_fastqc):\n",
    "                fastqc_status = wfr_resp.get('run_status')     \n",
    "            elif wfr_name.startswith(wf_partA):\n",
    "                last_part_A=wfr_resp['uuid']\n",
    "                last_part_A_status = wfr_resp.get('run_status')  \n",
    "                \n",
    "    # Check for md5 and fastqc, and if not complete, run or report it. \n",
    "    # if exclude miseq is on, do this only if sequencer is not miseq\n",
    "    if not exclude_miseq or sequencer != \"Illumina MiSeq\":\n",
    "        # check if md5 step is completed properly\n",
    "        \n",
    "        if status != \"uploaded\" or md5_status not in status_done:\n",
    "            # if not, shall we run it?\n",
    "            if run_md_qc in ['md5', 'all']:\n",
    "                print 'md5 running for', file_resp['accession']\n",
    "                code_md5= \"invoke run_md5 \" + env + \" \" + file_resp['accession'] + \" \" + file_resp['uuid']\n",
    "                run(code_md5)\n",
    "                print ''\n",
    "                time.sleep(10)\n",
    "            # user does not want it to be run, so just report\n",
    "            else:\n",
    "                print 'md5 run missing for', file_resp['accession']\n",
    "        # check fastqc if md5 is fine\n",
    "        else:\n",
    "            if not qc or fastqc_status not in status_done:\n",
    "                # if not, shall we run it?\n",
    "                if run_md_qc in ['qc', 'all']:\n",
    "                    print 'fastqc running for', file_resp['accession']\n",
    "                    code_qc= \"invoke run_fastqc \" + env + \" \" + file_resp['accession'] + \" \" + file_resp['uuid']\n",
    "                    run(code_qc)\n",
    "                    print ''    \n",
    "                    time.sleep(10)\n",
    "                # user does not want it to be run, so just report\n",
    "                else:\n",
    "                    print 'fastqc run missing for', file_resp['accession'], fastqc_status\n",
    "                    print \n",
    "   \n",
    "    # return a small report\n",
    "    return {'file': file_id,\n",
    "            'alias': first_alias,\n",
    "            'sequencer': sequencer,\n",
    "            'pair_no': pair_no,\n",
    "            'paired_file': paired_file,\n",
    "            'file_status': status,\n",
    "            'qc': qc,\n",
    "            'md5_status': md5_status,\n",
    "            'fastqc_status': fastqc_status,\n",
    "            'last_part_A': last_part_A,\n",
    "            'last_part_A_status': last_part_A_status\n",
    "           }\n",
    "\n",
    "\n",
    "report = []\n",
    "enzymes = []\n",
    "for exp in rep_resp:\n",
    "    # print 'Experiment', exp\n",
    "    print 'b'\n",
    "    exp_resp = ff_utils.get_metadata(exp, connection=ff)\n",
    "    print 'b'\n",
    "    exp_files = exp_resp['files']\n",
    "    enzyme = exp_resp['digestion_enzyme']\n",
    "    enzymes.append(enzyme)\n",
    "    for fastq_file in exp_files[:]:\n",
    "        print 'c'\n",
    "        print fastq_file\n",
    "        file_resp = ff_utils.get_metadata(fastq_file, connection=ff, frame='embedded')  \n",
    "        print 'c'\n",
    "        #Some checks before running\n",
    "        #check if status is deleted\n",
    "        if file_resp['status'] == 'deleted':\n",
    "            continue\n",
    "        #if no uploaded file in the file item report and skip\n",
    "        if not file_resp.get('filename'):\n",
    "            print file_resp['accession'], \"does not have a file\"\n",
    "            continue\n",
    "        # check if file is in s3\n",
    "        head_info = tibanna.s3.does_key_exist(file_resp['upload_key'], tibanna.s3.raw_file_bucket)\n",
    "        if not head_info:\n",
    "            print file_resp['accession'], \"does not have a file in S3\"\n",
    "            continue\n",
    "        \n",
    "        # skip pair no 2\n",
    "        if file_resp.get('paired_end')=='2':\n",
    "            continue \n",
    "            \n",
    "        # if the file size is more than 42 GB do not run\n",
    "        file_size = head_info['ContentLength']\n",
    "        if file_size > 45097156608:\n",
    "            print file_resp['aliases'], 'larger than 42GB, skipping, please use another script'\n",
    "            continue\n",
    "\n",
    "        file_info = summarize_file(file_resp)\n",
    "\n",
    "        # check for miseq\n",
    "        if exclude_miseq:\n",
    "            if file_info['sequencer'] == 'Illumina MiSeq':\n",
    "                continue\n",
    "        paired_file = file_info['paired_file']\n",
    "        pair_file_resp = ff_utils.get_metadata(paired_file, connection=ff, frame='embedded')\n",
    "        pair_file_info = summarize_file(pair_file_resp)\n",
    "\n",
    "        # check consistency of paired file info\n",
    "        # status differences gives error but there are multiple statuses that indicate complete\n",
    "        # TODO fix it\n",
    "        pairs_inconsistent = \"\"\n",
    "        check_items = [ i for i in file_info.keys() if i not in ['file', 'paired_file', 'pair_no', 'alias']]\n",
    "        for check_item in check_items:\n",
    "            try:\n",
    "                assert file_info[check_item] == pair_file_info[check_item]\n",
    "            except AssertionError:\n",
    "                print check_item, \"not the same between pair\", fastq_file, 'and', paired_file\n",
    "                pairs_inconsistent += check_item + ', '\n",
    "        wf_check = ''\n",
    "        # check if md5 and qc are okay\n",
    "        for info in [file_info, pair_file_info]:\n",
    "            if (info['md5_status'] in status_done and info['file_status'] == 'uploaded' and\n",
    "                info['fastqc_status'] in status_done and info['qc'] == True):\n",
    "                wf_check += '+'\n",
    "        rep = {\"consistency\": pairs_inconsistent,  \n",
    "               \"file1\": file_info['alias'],\n",
    "               \"file2\": pair_file_info['alias'],\n",
    "               \"const_check\": pairs_inconsistent,\n",
    "               \"wf_check\": wf_check, \n",
    "               \"partA_wf\": file_info['last_part_A'], \n",
    "               \"partA_status\": file_info['last_part_A_status']\n",
    "               }\n",
    "        # status differences gives error but there are multiple statuses that indicate complete\n",
    "        # TODO fix it\n",
    "        report.append(rep)\n",
    "        if rep.get('const_check'):\n",
    "            print rep['const_check']\n",
    "\n",
    "    \n",
    "# TODO need to add failed ones\n",
    "# 1 completed pairs\n",
    "pairs_completed = [i for i in report if i['partA_status']=='complete']\n",
    "# 2 running pairs\n",
    "pairs_running = [i for i in report if i['partA_status'] not in ['complete','did_not_run']]\n",
    "# 3 no run pairs\n",
    "pairs_did_not_run = [i for i in report if i['partA_status']=='did_not_run']\n",
    "# 3a no run pairs with fine qc md5\n",
    "pairs_ready_to_run = [(i['file1'], i['file2']) for i in pairs_did_not_run if i['wf_check'] == '++']\n",
    "# 3b no run pairs with problematic qc md5\n",
    "pairs_qcmd_problem = [(i['file1'], i['file2']) for i in pairs_did_not_run if i['wf_check'] != '++']\n",
    "\n",
    "# 2 running pairs to run again\n",
    "rerun_running_pairs = [(i['file1'], i['file2']) for i in report if i['partA_status'] not in ['complete','did_not_run']]\n",
    "\n",
    "rerun_started_pairs = [(i['file1'], i['file2']) for i in report if i['partA_status']=='started']\n",
    "\n",
    "\n",
    "print \"{}/{} pairs completed partA\".format(len(report), len(pairs_completed))\n",
    "print \"{}/{} pairs still running partA\".format(len(report), len(pairs_running))\n",
    "print \",\".join(i['partA_status'] for i in pairs_running)\n",
    "print ''\n",
    "print '1) ready to run (pairs_ready_to_run)'\n",
    "for a,b in pairs_ready_to_run:\n",
    "    print a,b\n",
    "print \"\"\n",
    "\n",
    "print '2) problematics ones (pairs_qcmd_problem)'\n",
    "for a,b in pairs_qcmd_problem:\n",
    "    print a,b\n",
    "print \"\"\n",
    "\n",
    "print '3) running ones (rerun_running_pairs)'\n",
    "for a,b in rerun_running_pairs:\n",
    "    print a,b\n",
    "print \"\"\n",
    "\n",
    "print '4) running ones (rerun_started_pairs)'\n",
    "for a,b in rerun_started_pairs:\n",
    "    print a,b\n",
    "print \"\"\n",
    "\n",
    "# Choose the right NZ reference file\n",
    "re_ref_file = ''\n",
    "choice = {'HindIII': '4DNFI823MBKE', 'MboI': '4DNFI823L812'}\n",
    "# Check if all experiments use the same enzyme\n",
    "if len(list(set(enzymes))) != 1:\n",
    "    print \"ERROR Mixed Enzyme Content in Experiment Set\"\n",
    "else:\n",
    "    nz_name = enzymes[0].split('/')[2]\n",
    "    re_ref_file = choice[nz_name]\n",
    "print 'using {} ({}) as the enzyme'.format(nz_name, re_ref_file)\n",
    "print \"DONE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "import time\n",
    "\n",
    "# hic-partA, paired files\n",
    "#paired_files = [('file1.fastq.gz', 'file2.fastq.gz')]\n",
    "#pairs_qcmd_problem\n",
    "#pairs_ready_to_run\n",
    "#rerun_running_pairs\n",
    "\n",
    "paired_files = rerun_running_pairs\n",
    "\n",
    "# testportal\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "outfiles = tibanna.s3.outfile_bucket\n",
    "tibanna.s3.outfile_bucket = 'elasticbeanstalk-fourfront-webdev-files'\n",
    "\n",
    "# todo need a function to determin this given fastq1\n",
    "index = make_input_file_json('4DNFIZQZ39L9', 'bwa_index', tibanna)\n",
    "chrsizes = make_input_file_json('4DNFI823LSII', 'chrsizes', tibanna)\n",
    "ref = make_input_file_json('4DNFI823L888', 'reference_fasta', tibanna)\n",
    "restrict = make_input_file_json(re_ref_file, 'restriction_file', tibanna)\n",
    "ncores = 32   \n",
    "\n",
    "# ncore options 8  cores up to 20gb per .fastq.gz (1tb)\n",
    "#               36 cores up to 42gb per .fastq.gz (2tb)\n",
    "#     Not set            up to 90gb per .fastq.gz (4tb)\n",
    "\n",
    "for pair in paired_files:\n",
    "    fastq1 = make_input_file_json(pair[0], 'fastq1', tibanna)\n",
    "    fastq2 = make_input_file_json(pair[1], 'fastq2', tibanna)\n",
    "    \n",
    "     \n",
    "    input_files = [fastq1, fastq2, index, chrsizes, ref, restrict]\n",
    "    if all(input_files):\n",
    "        name = fastq1['object_key'].split('.')[0] + \"-\" + fastq2['object_key'].split('.')[0]\n",
    "        input_json = make_hica_json(input_files, env, outfiles, name, ncores)\n",
    "        res = run_workflow(input_json)\n",
    "        print input_json\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    time.sleep(30)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter]",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
