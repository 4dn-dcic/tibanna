{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triage Workflow Run Error\n",
    "\n",
    "This notebook will look through Tibanna's StepFunction interface on AWS, shows some graphs, then determine the last failed execution and show you what the error for the last execution was.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43b2ff5ff021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'load_ext autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'autoreload 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pandas"
     ]
    }
   ],
   "source": [
    "# first we just load up some stuff\n",
    "# %load init.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core import utils\n",
    "import boto3\n",
    "import json\n",
    "pd.set_option('display.max_columns',100);pd.set_option('display.max_rows',1000)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to AWS and see what's going on\n",
    "client = boto3.client('stepfunctions', region_name='us-east-1')\n",
    "base_arn = 'arn:aws:states:us-east-1:643366669028:%s:%s'\n",
    "STEP_FUNCTION_ARN = base_arn % ('stateMachine', 'run_sbg_workflow_2')\n",
    "\n",
    "executions = client.list_executions(stateMachineArn=STEP_FUNCTION_ARN, maxResults=1000)\n",
    "exec_df = pd.DataFrame(executions.get('executions'))\n",
    "exec_df.sort_values(by='startDate', ascending=False, inplace=True)\n",
    "exec_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean up the names of runs so we can better group things for graphing\n",
    "\n",
    "to_replace = ['fastqc*', 'validate*', 'test*']\n",
    "value = ['fastqc', 'validate', 'testing']\n",
    "exec_df['type'] = exec_df.name.replace(to_replace='[-,_].*$', value='', regex=True, inplace=False)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,3))\n",
    "\n",
    "group_by_status = exec_df.groupby(['type', 'status'])\n",
    "group_by_status.size().unstack().plot(kind='bar', stacked=True, ax=axs[0])\n",
    "\n",
    "overview = exec_df.groupby(['status'])\n",
    "overview.size().plot(kind='bar', stacked=True, ax=axs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the diagnostics.  Below you can use num_days to shorten the length of the query which will show you all the errors found in that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failures in last num_days\n",
    "num_days = 10\n",
    "import datetime\n",
    "cutoff = datetime.datetime.today() - datetime.timedelta(days=10) \n",
    "latest = exec_df.loc[(exec_df.startDate > cutoff) & (exec_df.status.apply(lambda x: x in ['FAILED',]))]\n",
    "print(\"%s errors found for all step functions since %s\" % \n",
    "      (len(latest), cutoff.date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default we will get the most recent error, but you can set exec_name if you want to look at a particular execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default though we get the most recent errror\n",
    "exec_name = latest.iloc[0]['name']\n",
    "\n",
    "# use to view errors for a specific step-function run result\n",
    "# exec_name = 'fastqc-0-11-4-1-1_4DNFIWJZJ6JS.fastq.gz'\n",
    "\n",
    "def get_exec_arn(name):\n",
    "    try:\n",
    "        return latest.loc[latest.name == name, 'executionArn'].iloc[0]\n",
    "    except:\n",
    "        print(\"run %s not found \" % name)\n",
    "        return ''\n",
    "       \n",
    "def get_exec_url(name):\n",
    "    try:\n",
    "        arn = latest.loc[latest.name == name, 'executionArn'].iloc[0]\n",
    "        baseurl = 'https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/'\n",
    "        return '%s%s' % (baseurl, arn)\n",
    "    except:\n",
    "        print(\"run %s not found\" % name)\n",
    "        return ''\n",
    "    \n",
    "url = get_exec_url(exec_name)\n",
    "arn = get_exec_arn(exec_name)\n",
    "print(\"you can view the url for the failing run by clicking on the link below:\\n\")\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see which lambda for the run failed\n",
    "resp = client.get_execution_history(executionArn=arn, reverseOrder=True)\n",
    "\n",
    "# find the failing lambda\n",
    "for i, event in enumerate(resp.get('events')):\n",
    "    if event.get('type') == 'LambdaFunctionScheduled':\n",
    "        break\n",
    "        \n",
    "failing_lambda = resp.get('events')[:i+1]\n",
    "lambda_arn = failing_lambda[-1]['lambdaFunctionScheduledEventDetails']['resource']\n",
    "input_json = failing_lambda[-1]['lambdaFunctionScheduledEventDetails']\n",
    "lambda_time = failing_lambda[-1]['timestamp']\n",
    "failure = failing_lambda[0].get('executionFailedEventDetails')\n",
    "if failure is None:\n",
    "    failure = failing_lambda[0].get('executionAbortedEventDetails')\n",
    "\n",
    "print(\"lambda %s \\nRunning at %s failed \\nFailure is %s\" % \n",
    "      (lambda_arn, str(lambda_time), failure))\n",
    "print\n",
    "print\n",
    "print(\"input json for the lambda was:\") \n",
    "print\n",
    "print(input_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(failure['cause'])['errorMessage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what follows is the cloudwatch logs if we can find any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awslambda = boto3.client('lambda', region_name='us-east-1')\n",
    "lambda_details = awslambda.get_function_configuration(FunctionName=lambda_arn)\n",
    "print(\"lambda was last updated %s UTC time\" % \n",
    "      lambda_details['LastModified'])\n",
    "\n",
    "\n",
    "# need to conver to format aws likes\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "epoch = pytz.utc.localize(datetime.datetime.utcfromtimestamp(0))\n",
    "\n",
    "def unix_time_millis(dt):\n",
    "    return int((dt - epoch).total_seconds()) * 1000\n",
    "\n",
    "log_stream = \"/aws/lambda/%s\" % lambda_details['FunctionName']\n",
    "\n",
    "cloudwatch = boto3.client('logs', region_name='us-east-1')\n",
    "end_time_local = failing_lambda[0]['timestamp']\n",
    "\n",
    "end_time = unix_time_millis(end_time_local.astimezone(pytz.utc))\n",
    "\n",
    "start_time_local = failing_lambda[-1]['timestamp']\n",
    "start_time = unix_time_millis(start_time_local.astimezone(pytz.utc))\n",
    "cloudwatch.filter_log_events(logGroupName=log_stream,\n",
    "                            startTime=start_time,\n",
    "                            endTime=end_time)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some of the checks below should help identify common problems, such as file doesn't exist..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: ensure workflow exists on target server\n",
    "\n",
    "# TODO: check the input files exists on s3\n",
    "\n",
    "# TODO: validate input for step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
