{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core.utils import Tibanna\n",
    "from core import ff_utils\n",
    "import copy\n",
    "\n",
    "env = 'fourfront-webprod'\n",
    "tibanna = Tibanna(env=env)\n",
    "ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "\n",
    "def sort_p_files(p_file_list, acc):\n",
    "    archive = []\n",
    "    new_p_files = copy.deepcopy(p_file_list)\n",
    "    deleted = []\n",
    "    \n",
    "    for p_file in p_file_list:\n",
    "        p_file_resp = ff_utils.get_metadata(p_file, connection = ff, frame= 'embedded')\n",
    "        \n",
    "        # get rid of all deleted files\n",
    "        if p_file_resp['status'] == 'deleted':\n",
    "            deleted.append(p_file)\n",
    "            continue\n",
    "\n",
    "        p_wfr =  p_file_resp['workflow_run_outputs']\n",
    "\n",
    "        assert len(p_wfr) == 1\n",
    "        wfr_name = p_wfr[0]['display_title']\n",
    "\n",
    "        if '0.2.5' not in wfr_name:\n",
    "            archive.append(p_file)\n",
    "            new_p_files.remove(p_file)\n",
    "    \n",
    "    rep = 'no change'\n",
    "    if archive:       \n",
    "        rep = 'proc_files will be archived'\n",
    "        if new_p_files:\n",
    "            rep += ' partially'\n",
    "        if deleted:\n",
    "            rep += ', cleaned deleted items'\n",
    "    elif deleted:\n",
    "        rep = 'cleaned deleted items'\n",
    "    \n",
    "    \n",
    "        # add archived\n",
    "        patch_data_1 = {'archived_processed_files': archive}\n",
    "        try:\n",
    "            ff_utils.patch_metadata(patch_data_1, obj_id=acc ,connection=ff)\n",
    "        except Exception as e:\n",
    "            print e \n",
    "        \n",
    "        # update processed files\n",
    "        patch_data_2 = {'processed_files': new_p_files}\n",
    "        try:\n",
    "            ff_utils.patch_metadata(patch_data_2, obj_id=acc ,connection=ff)\n",
    "        except Exception as e:\n",
    "            print e \n",
    "    return archive, new_p_files, rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on all experiment sets\n",
    "\n",
    "url = '/search/?status=released&status=released%20to%20project&status=archived&status=submission%20in%20progress&type=ExperimentSetReplicate'\n",
    "sets = ff_utils.get_metadata(url, connection=ff)['@graph']\n",
    "print len(sets), 'experiment sets'\n",
    "pros_sets = [i for i in sets if i.get('processed_files')]\n",
    "print len(pros_sets), 'sets with processed files'\n",
    "stati = [i['status'] for i in sets if i.get('processed_files')]\n",
    "print 'with statuses', \",\".join(list(set(stati))).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# work on experiment set part 2\n",
    "for a_set in pros_sets:\n",
    "    exps = a_set['experiments_in_set']\n",
    "    p_files = a_set['processed_files']\n",
    "    # take care of sets first\n",
    "    arch, new_p, rep = sort_p_files(p_files, a_set['accession'])\n",
    "    print 'set', a_set['accession'], rep\n",
    "    \n",
    "    for exp in exps:\n",
    "        p_files_exp = ff_utils.get_metadata(exp, connection = ff)\n",
    "        arch_e, new_p_e, rep_e = sort_p_files(p_files_exp['processed_files'], p_files_exp['accession'])\n",
    "        print 'exp', p_files_exp['accession'], rep_e\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on individual types of experiments\n",
    "\n",
    "exp_type = 'ExperimentHiC'\n",
    "url_hic = '/search/?status=released&status=released%20to%20project&status=archived&status=submission%20in%20progress&type='+exp_type\n",
    "hic_exps = ff_utils.get_metadata(url_hic, connection=ff)['@graph']\n",
    "print len(hic_exps), 'experiment hic'\n",
    "\n",
    "pros_hic = [i for i in hic_exps if i.get('processed_files')]\n",
    "print len(pros_hic), 'hics with processed files'\n",
    "\n",
    "stati_hic = [i['status'] for i in hic_exps if i.get('processed_files')]\n",
    "print 'with statuses', \",\".join(list(set(stati_hic))).upper()\n",
    "\n",
    "for a_hic in pros_hic:\n",
    "    p_files = a_hic['processed_files']\n",
    "    # take care of sets first\n",
    "    arch, new_p, rep = sort_p_files(p_files, a_hic['accession'])\n",
    "    print 'hic', a_hic['accession'], rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
