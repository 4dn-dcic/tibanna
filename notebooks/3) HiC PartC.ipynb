{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hicc_json(input_files, env, output_bucket, accession, ncores):\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': output_bucket,\n",
    "                  'workflow_uuid': \"1348354f-49e5-4c33-afab-9ec90d65faf3\",\n",
    "                  \"app_name\": \"hi-c-processing-partc/2\",\n",
    "                  \"parameters\": {\n",
    "                      \"ncores\" :  ncores\n",
    "                  },\n",
    "                  \"_tibanna\": {\"env\": env, \"run_type\": \"hic_part_c\", \"run_id\": accession}\n",
    "                  }\n",
    "    return input_json\n",
    "\n",
    "def make_input_file_json(obj_ids, arg_name, tibanna, bucket):\n",
    "    '''\n",
    "    obj_ids can be either a string or a list.\n",
    "    {\n",
    "      \"bucket_name\": \"%s\",\n",
    "      \"object_key\": \"%s\",\n",
    "      \"uuid\" : \"%s\",\n",
    "      \"workflow_argument_name\": \"%s\"\n",
    "    }\n",
    "    '''\n",
    "    ff = ff_utils.fdn_connection(key=tibanna.ff_keys)\n",
    "    \n",
    "    if not isinstance(obj_ids, list):\n",
    "        obj_ids = [ obj_ids ]\n",
    "        \n",
    "    object_key_list = []\n",
    "    uuid_list = []\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        metadata = ff_utils.get_metadata(obj_id, connection=ff)\n",
    "         \n",
    "        # just make sure the file is on s3, otherwise bail\n",
    "        print(\"looking for upload key %s, on bucket %s\" % \n",
    "              (metadata['upload_key'],\n",
    "               bucket))\n",
    "        if tibanna.s3.does_key_exist(metadata['upload_key'], bucket=bucket):\n",
    "            object_key_list.append(metadata['upload_key'].split('/')[1])\n",
    "            uuid_list.append(metadata['uuid'])\n",
    "            \n",
    "    if len(uuid_list)==1:\n",
    "        uuid_list = uuid_list[0]\n",
    "    if len(object_key_list)==1:\n",
    "        object_key_list = object_key_list[0]\n",
    "        \n",
    "    data = {'bucket_name' : bucket,\n",
    "            'object_key' :  object_key_list,\n",
    "            'uuid' : uuid_list,\n",
    "            'workflow_argument_name': arg_name\n",
    "            }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for upload key 439cd2d1-9586-4942-b3cd-8b0ea5971206/4DNFIIIHUUHH.cool, on bucket elasticbeanstalk-fourfront-webdev-wfoutput\n",
      "{'parameters': {'ncores': 8}, '_tibanna': {'run_type': 'hic_part_c', 'env': 'fourfront-webdev', 'run_id': 'Jin_et_al_4DNFIIIHUUHH'}, 'output_bucket': 'elasticbeanstalk-fourfront-webdev-wfoutput', 'input_files': [{'workflow_argument_name': 'input_cool', 'bucket_name': 'elasticbeanstalk-fourfront-webdev-wfoutput', 'uuid': u'439cd2d1-9586-4942-b3cd-8b0ea5971206', 'object_key': u'4DNFIIIHUUHH.cool'}], 'workflow_uuid': '1348354f-49e5-4c33-afab-9ec90d65faf3', 'app_name': 'hi-c-processing-partc/2'}\n",
      "about to start run hic_part_c_Jin_et_al_4DNFIIIHUUHH\n",
      "response from aws was: \n",
      " {u'startDate': datetime.datetime(2017, 7, 18, 16, 4, 44, 452000, tzinfo=tzlocal()), 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '579d2d2c-6bf4-11e7-8dd3-7d1021684ac9', 'HTTPHeaders': {'x-amzn-requestid': '579d2d2c-6bf4-11e7-8dd3-7d1021684ac9', 'content-length': '148', 'content-type': 'application/x-amz-json-1.0'}}, u'executionArn': u'arn:aws:states:us-east-1:643366669028:execution:run_sbg_workflow_3:hic_part_c_Jin_et_al_4DNFIIIHUUHH'}\n",
      "url to view status:\n",
      "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:run_sbg_workflow_3:hic_part_c_Jin_et_al_4DNFIIIHUUHH\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from core.utils import Tibanna\n",
    "from core.utils import run_workflow\n",
    "from core import ff_utils\n",
    "import time\n",
    "\n",
    "# testportal\n",
    "env = 'fourfront-webdev'\n",
    "tibanna = Tibanna(env=env)\n",
    "\n",
    "#rao13 cool file 4DNFIT41X0MQ\n",
    "#jin   cool file 4DNFIIIHUUHH\n",
    "all_cools = [\n",
    "['/files-processed/4DNFIIIHUUHH/',\"Jin_et_al\"]\n",
    "]\n",
    "\n",
    "\n",
    "output_file_bucket = tibanna.s3.outfile_bucket\n",
    "raw_file_bucket = tibanna.s3.raw_file_bucket\n",
    "\n",
    "for cool_accession, set_name in all_cools:\n",
    "    cool_file= make_input_file_json(cool_accession, 'input_cool', tibanna, output_file_bucket)\n",
    "    ncores = 8\n",
    "\n",
    "    input_files = [cool_file]\n",
    "    if all(input_files):\n",
    "        name = set_name + '_' + cool_accession.split('/')[2]\n",
    "        input_json = make_hicc_json(input_files, env, output_file_bucket, name, ncores)\n",
    "        print input_json\n",
    "        res = run_workflow(input_json)\n",
    "    else:\n",
    "        print(\"some files not found on s3.  Investigate this list %s\" % input_files)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
